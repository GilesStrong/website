{
  
    
        "post0": {
            "title": "Into the world of headphone audio",
            "content": ". I love music, and often associate songs or entire albums to particular memories and places, like school runs with my Dad listening to Pink Floyd’s Wish You Were Here, hearing Rammstein for the first time, or simply a dinner at home one evening listening to Mike Oldfield’s Crises. . Aside from the odd CD, the majority of music I grew up with was on records, and part of what I associate with music is the tactile sense of doing something to get the music to play, i.e. placing the needle on the record and having to flip the record half way through the album. Nowadays I substitute this by buying CDs and placing them in my computer, even though I have a perfectly good lossless rip of the same CD on the hard drive. Given the fact that I’ll generally listen to several hours of music a day, it is strange that my audio setup has otherwise been quite lacking in terms of performance, and recently I have been making efforts to replace and upgrade it. . Like anything I plan to buy which involves an investment, I aim to research and understand as much as I can, so I am able to purchase the most suitable equipment for what I need. The world of personal audio, however, turned out to be quite deep, so in this post I aim to share what I learnt and how my final setup is performing. As a warning up front, I won’t be covering speakers here. . Starting setup . I’ve already gone through a few different bits of audio equipment, but my setup before this upgrade was: . A pair of NAD Viso HP-50 headphones - closed back, dynamic driver, ~£140, on their third set of pads (PU leather ATH M50X pads from Wicked Cushions). Perfectly usable and enjoyable to listen to. . | 1more Triple Driver in-ear monitors - dynamic+balanced armature drivers, ~100 euro, Inair memory foam tips. Had a rather grating treble spike which smoothed a bit after usage, but never truly went away. . | Beats Solo 3 headphones - closed back, dynamic driver, free with a MacBook. Too bassy to use. . | Creative USB soundcard for PC usage - 24bit, 192KHz. . | iPhone SE 2020 + lighting to 3.5mm dongle - synced with my PC music library. . | MaCBook Pro 13” 2018 - 3.5mm jack for Spotify streaming. . | . Normally, I’ll listen to my music library, which is split 20:80 lossless : MP3/AAC, with the majority of the lossless being 16bit 44.1 kHz CD rips in ALAC. I’m starting to get a few hi-res files, but don’t have many. Whether or not these numbers make a perceivable difference isn’t something I’ve really sat down to test (maybe I’ll run an experiment and report back). My main thinking is, if I spend £10-15 on a CD, I want to be able to listen to the same music regardless of whether I have a CD drive available. . . Pictured: my well-worn and trusty NAD Viso HP-50. The earpads were recently replaced. . In looking to upgrade my setup: I was happy with my HP-50 but was also curious to see what other headphones sounded like, which may need an amplifier; and I wanted to replace the Triple Drivers for something less fatiguing. Also, what I hadn’t realised is that the lighting port on the iPhone can only output a digital signal, and all the conversion to analogue takes place in a tiny chip inside the dongle, rather than using the full processor in the iPhone. I was therefore a bit concerned about whether this could be a bottleneck. . Headphones . Headphones have several qualities: . Frequency response - how the output level varies with the sound frequency; . | Speed - how quickly the headphones can respond to sudden changes in the source; . | Imaging - how well different instruments/sound sources can be placed and separated; . | Soundstage - how well the headphones can give a sense of the recording space and distance between source and listener; . | Isolation/leakage - how easily outside sounds can be heard, and how easily headphone sounds can be heard by other people. . | . Headphones can roughly be divided according to: . Whether they fit over the ear or are placed on top of it. This mainly affects comfort and portability so I won’t talk about it here. . | Whether they are closed-back or open-back; . | How they produce the sound. . | . Open or closed back . The choice between open or closed back makes a big change by affecting the soundstage of the headphones. This is honestly one of the more confusing things I’ve struggled to get my head around, since there are varying explanations and interpretations of what soundstage is. The two conflicting explanations I have heard are: . Soundstage is inherent to the recording, by capturing echos and ambient sounds of the recording space. When listening to headphones, sound waves which do not enter the ear bounce back, and in closed-back headphones, the sound is then reflected back to the ear. This false echo leads to a muddying of the sound and a degradation of the illusion of distance. With open-back headphones more of the reflected sound is able to escape, leading to a cleaner sound, and helps to better create a soundstage, since the only echos heard are those in the recording. . | Soundstage is something artificial and entirely created by the headphones themselves, i.e. it is not inherent to the recording. It is more easily created in open-back headphones, since the escaping sound can interact with the listener’s environment and be reflected back, or even into the opposite ear. This artificially makes it seem like the sounds are produced further from the listener. . | Personally, I think the first explanation is more correct: I can listen to two different tracks (e.g. a live recording and a studio recording) with the same (open-back) headphones in the same location and both tracks have a different soundstages (the live recording has more of a sense of distance, whereas the studio track as been mixed to provide a more casual listening experience), but according to the second explanation, both should have similar soundstages. In either case, though, the consensus is that open-back provides a better soundstage. The downsides are that they have very poor isolation and high leakage, and can struggle to produce very-low bass frequencies due to the lack of a seal around the ears . Driver . Headphones produce sound by using a diaphragm to shift air at different frequencies. The overall configuration of the diaphragm is referred to as the driver, and several common types exist: . Dynamic driver: The easiest to produce and most commonly found driver, in which a plastic cone diaphragm is moved by a solenoid. Their advantage is that they are cheap to build and can be manufactured to have a more consistent response in terms of volume and frequency, which improves imaging. The downside is that as the diaphragm moves, its shape changes causing an alteration of the sound. . | Planar magnetic: Here the diaphragm is a flat sheet with a thin electrical substrate through which the audio signal is passed. Surrounding the diaphragm are a pair of permanent magnets, causing the diaphragm to move with the signal. The lower mass of the diaphragm means it is able to react more quickly to changes in the music (e.g. when a sound starts or stops playing), sometimes called transient response. The lower change in shape during movement also leads to lower distortion of the sounds. Unfortunately, the drivers are more difficult to manufacture consistently, which can slightly affect imaging. . | Electrostatic: How can we further reduce the diaphragm mass? By also removing the metal wiring. Electrostatic headphones require an “energiser” which applies high voltages to perforated plates placed either side of a very thin, statically charged diaphragm, causing it to move according to the signal. These further extend in advantages and disadvantages of planar magnetics, and are some of the most expensive headphones available (e.g. Sennheiser Orpheus at a whopping $60,000, more budget models exist, though), thanks to the need for the bulky, standalone energiser, and the precision required to manufacture the diaphragms. . | IEMs . In-ear monitors (IEMs) can also have a variety of drivers, and sometimes multiple (e.g. my 1more Triple Drivers) with each focussing on different frequencies. Generally, the soundstage is quite poor, but they can provide better passive isolation due to entering into the ear canal and are very portable. The sound performance is generally worse than similarly priced headphones, except perhaps below a budget of ~100 euros. . Amplification . Headphones drivers convert the electrical sound into sound waves however if the supplied signal is not strong enough then the headphones will not be able to produce sound as loudly or as correctly as they should. An amplifier can be used to increase the analogue signal strength to drive more demanding headphones. Two factors of the headphone determine how strong the input needs to be to drive them: their impedance (Ω Ohms) - electrical resistance; and their sensitivity (dB@1mW) - loudness at a specific signal strength. The higher the impedance and the lower the sensitivity, the greater the power required to drive them. . From my understanding, high-impedance headphones are slightly more accurate in their sound, but are really designed for working with studio equipment - signals are generally higher and accidentally plugging 32Ω headphones into a line-level output could damage the listener’s ears. Indeed, companies like Beyerdynamic even sell different impedance versions of the same headphone models. Sensitivity generally varies between headphone types, e.g. my Triple Drivers sound much louder than my HP-50 even though they are both 32Ω impedance, similarly a 37Ω set of planar magnetic headphones sound much quieter due to reduced sensitivity even though the impedance as only changed slightly. A calculator for computing the requirements of a set of headphones can be found here. Of course if the audio source is already sufficient for the headphones then an amplifier is not strictly necessary, however it is always good to have decent headroom in the supply to ensure the headphones are always sufficiently driven across the whole frequency range. . Headphone amplifiers vary considerably in form-factor, ranging from tiny palm-sized or pocket-size amps, to larger desktop units. On top of whether the amp provides a sufficient output signal, one must also consider whether the amp adds distortion to the signal, or preferentially amplifies signals in certain frequency regions. Amps are either solid state, or valve driven. The former is more clean and suited for analytic listening, whereas the latter introduces a pleasant distortion which warms the sound, favouring casual listening. The general recommendation I’ve seen is to at least buy solid state first. . Standalone amps usually provide dual RCA inputs for left and right audio channels, and perhaps dual 3-pin XLR inputs. As outputs they’ll normally have either a 3.5mm or 6.35mm single-ended headphone jacks, and perhaps either a 4-pin XLR, 4.4mm, or 2.5mm balanced headphone jacks. Additionally there will usually be a volume control affecting the amplifier gain. . Digital to analogue converters . The source signal for audio is likely to be digital, and so must be converted into an analogue signal before it can be played, this is achieved via a digital to analogue converter (DAC) chip. Similar to amps, DACS can vary considerably in scale from the iPhone dongle, to desktop units.The chips tend to be manufactured by a handful of third-party companies, and in theory DAC units should function equally well, however differences in other components can cause changes in their sound. Additionally, some DACs provide options to alter the filtering of high frequencies, EQ the sound, and add harmonic distortions to the sound (e.g. to simulate a valve amp). . The main consideration I think is the inputs to the DAC; most offer USB, but some also have bluetooth, optical, and coaxial inputs. It is also possible to buy complete DAC+Amp units. Mainly this is for portable convenience, but there are also a few desktop-sized combo units. . Upgrading my audio . Since my HP-50 headphones are a closed-back and dynamic-driver, I wanted to get some open-back planar magnetic headphones, which I thought would need an amplifier, and a replacement set of IEMs for travelling and office. I also thought I’d buy a desktop DAC, since my soundcard was quite old, and from a company primarily associated with gaming so might be slightly EQed for that. When they arrived, however, and I heard how much better even my old headphones sounded through the setup, and how underpowered the new headphones sounded through the iPhone dongle, I decided to also buy a portable DAC/Amp. . Headphones . For headphones, I generally prefer a flatter response, meaning that no particular frequencies are emphasised. This makes them suitable for a wider range of music and doesn’t alter their sound too much from what the producer intended. I also occasionally mix and master music using headphones, so need to have something that can act as a decent reference. Having owned my HP-50 for 5 years and already had to change both the cable and pads twice, it’s also a requirement of mine to be able to swap out parts of the headphones. . Based on these criteria, and the fact that I at least wanted open-back headphones, for a budget of 400 euros there seemed to be 3 sets that I saw recommended time after time: . Beyerdynamic DT1990-pro - dynamic driver &amp; 250Ω 102dB@1mW, ~370 euros . | HifiMan Sundara - planar magnetic &amp; 37Ω 94dB@1mW, ~350 euros . | Sennheiser HD660s - dynamic driver &amp; 150Ω 104dB@1mW, ~370 euros . | I wasn’t very taken by the styling of the HD660s, which looked very cheap and plasticy, so that was out. Comparing the Sundara and DT1990 and reading reviews for both, the Sundara had a measurably better response and soundstage and was easier to drive. That, coupled with the fact they are planar magnetic, meant that they were the clear choice. HifiMan had also recently revised the model, giving a slight improvement in the bass, and upgrading the earpads. . . Pictured: HifiMan Sundara. The comfortable hybrid earpads slope to be thicker at the back to fit better and to account for the unidirectional swivel of the earcups. The sellotape on the jack stops the signal from cutting out… . IEMs . For IEMs, I wanted to avoid something with a harsh treble, since that was why I wanted to replace my Triple Drivers, but didn’t want to spend too much due to the lower usage they’d get. Also, I didn’t anything with a particularly wide soundstage, for fear of constantly having to check if they were actually plugged in and I wasn’t listening to the speakers turned up loudly: I’d previously has some earbuds with a decent soundstage which I got used to, and once accidentally subjected a moderately full commuter train from Glasgow to Edinburgh to the first half of Amorphis’ 1992 album The Karelian Isthmus, an admittedly rather fine example of early Finnish death metal, due to not noticing that the jack wasn’t fully plugged into my phone. . Luckily, in the ~&lt;100 euro category, there were a few good choices (Blon BL-03, Tin T2, Moondrop Aria/Starfield) and after reading reviews, I opted for the Moondrop Starfield (110 euros), which seemed to have wide acclaim. The cables on them were also detachable, to add to their longevity. . . Pictured: the very colourful Moondrop Starfields, with Inairs Air2 foam tips. . Amplifier . Desktop amps vary considerably in price and capability. I was looking for something decent, which had enough power to future-proof me, but also was still on the budget-side. The Schiit Magnius was highly recommended, but I couldn’t find it for sale in Europe. Instead, the Topping A50s and SMSL SH-9 were also popular. The SH-9 had slightly better reviews, greater output power, a remote control, and supported balanced inputs, so I went for that at 290 euros. . Note: since I also planned to buy a desktop DAC, I had been looking at the Monoprice Monolith, which is a DAC/Amp combo, but again, couldn’t find it in stock in Europe. . DAC . Most companies produce amps and DACs in pairs. The Schiit Modius would have been a good choice but wasn’t available. The counterparts for the Topping and SMSL amps, D50s, and SU-9, also got good reviews and both had bluetooth. I went for the SU-9 (400 euros) since it also had balanced outputs, would work with my amp’s remote control, and had the option to add different styles of harmonic distortion to the output. . . Pictured: SMSL DAC/Amp stack. The same remote can be used for both items. . Portable DAC/Amp . I bought this later after hearing how much better my headphones sounded when powered by a proper amp. Primarily it’s for use when not sat at my PC, and to take with me when I travel. . There are plenty of phone-sized rechargeable DAC/Amps, such as the FiiO Q3 and iFi Hip-DAC. These can connect to phones and computers via USB. One of the main use cases I have, though, is listening whilst cooking, for which I’d have to carry both items in my pocket, and carefully extract both when wanting to rate or change a song. Also, since not all models were Apple certified, they would have to be connected via the Apple camera connection kit to get around hardware certification. . Instead I looked at lighter-weight models which supported bluetooth and so would allow me to have my phone out and more easily accessible. The FiiO BTR5 (140 euros) seemed to fit my needs well, and even has on-board volume and play/pause/skip controls. It can still connect via USB, too (with Apple CCK as needed). . ​​ . Pictured: The FiiO BTR5 portable bluetooth DAC/Amp, and the Apple Lightning-to-3.5mm dongle. The BTR5 has a few options (filters and EQ), and these are controllable via an app, or on device. . Testing . DAC &amp; Amp . What surprised me the most with the new amplifier (SMSL SH-9) is how much it improved the sound of my HP-50. Whilst they could certainly get loud enough through my USB soundcard, with the amplifier they seemed to have a much faster attack and decay, meaning that instruments like toms sounded fuller and had more punch, intricate playing on cymbals had a greater clarity to it, and even the different stages of a guitar-string pick were occasionally audible on distorted guitars. . The overall effect was that music sounded more vibrant and dynamic. An example might be Dark Tranquility’s 2002 album Damage Done; I love the guitar riffs on the album, but always felt that the overall sound was very flat and compressed. With the amp, though, the sound has more depth to it and is so much more exciting. Similarly with other albums and tracks; being able to listen into the music due to each instrument being more refined, I’ve discovered layers which I never noticed before, despite listening to some of the tracks almost 100 times. . The DAC (SMSL SU-9) made a less noticeable difference in the sound (my USB soundcard has RCA outputs so I can compare the DACs both into the same amp). The soundcard is a bit muffled, but I doubt I could tell in a blind test. The main benefit is the bluetooth connection, since now when working on my laptop, I can hook up my iPhone and still have access to all my music, albeit at a lower quality. That said, I’ve not actually noticed an immediately perceivable drop in quality when listening via bluetooth. The SMSL SU-9 provides options to colour the sound with harmonic distortion. This is quite subtle, though. It also provides control over the type of filter used to remove extremely high frequencies, but I couldn’t hear a difference between the options; perhaps because the music tracks have already been properly filtered. . I was a bit worried about the FiiO BTR5 portable bluetooth DAC/Amp, given that it was relatively cheap, and lower power than similar options, but quite honestly it’s amazing and I’m really glad I went for it. It’s a lot smaller than I expected, but has a nice feel to it, and its light weight means that I can clip it onto my clothes and have easy access to the controls. The phone I can leave either in my pocket, or on the counter if cooking to check songs etc. . Comparing the BTR5 and the dongle, the former is much more exciting to listen to; I always found with the dongle there was a certain volume level at which the headphones really “bit-in” and until then the sound was lacking. Unfortunately, the point at which this happened was uncomfortably loud for long periods of listening. The BTR5 achieves this point at a much lower volume. It also does something with the “shape” of the sound, but it is a bit hard for me to describe: the dongle seems to provide a “wall”, where all instruments and frequencies are presented flatly at once, whereas the BTR5 (and the SMSL stack) instead presents them like the seating at an amphitheatre (not in terms of size, but the sounds have more of a “slope” to them, for want of a better analogy). The one bad thing is that the bluetooth connection is less consistent than the SU-9’s, and sometimes songs will cut out, but only for less than a second. . Headphones . The HifiMan Sundara was my first experience with both open-back and planar magnetic headphones, and I was looking forward to the supposed “larger soundstage”, which open-backs are meant to offer. It seems, though, to be more of a subtle thing, and highly dependent on the recording. For most music (I generally listen too), the HP-50 places sounds against the outside of my head, whereas the Sundara places them in a cloud surrounding my head; instruments are still well defined in terms of angular resolution, but they can be slightly nearer or further away. Certain tracks, though, which have purposefully been recorded to create a sense of space make a much more noticeable difference. One example is the intro of Micheal Jackson’s Thriller, in which a door opens and someone walks from right to left: on the HP-50 the door opens inside my head and the footsteps follow a shallow semi-circle around my head from one ear to the other. The footsteps get quieter in the middle, indicating that they should be placed further away, but they don’t feel further away. Instead, on the Sundara, the door opens around my head, and the footsteps follow a deep bell-shaped curve, in the middle of which the footsteps do sound further away not just because of the lower volume. Another example is the live recording of Strive by Amber Rubarth: on the HP-50, the instruments do sound further away, and the reflections from the drum help give a sense of the recording space, but the Sundara instead puts you inside the room, surrounded by the instruments. . In terms of frequency response and speed: the Sundara has a much cleaner bass, but unfortunately, it lacks the same exciting nature of the HP-50 and whilst the toms etc. still have the same improved attack, they miss a bit of umph, perhaps due to the lower levels of sub-bass. This could be fixed with some EQ, but I prefer to keep everything “plug and play”. Instead what the Sundara offers is an overall more balanced and open presentation of the music, with different instruments and layers being more clearly identifiable. . One major disappointment with the Sundara is the stock cable, which is just some wire stuffed in a brittle plastic tube that is more prone to crease than bend. The signal would also keep cutting out until I reinforced the jack with sellotape. Of course it can be replaced, but this shouldn’t be a requirement for customers. . I think both sets of headphones have something to offer, and neither replaces the other. Instead I pick the set which most enhances the music I plan to listen to: if the music is exciting because it his hard-hitting and fun, I’ll go for the HP-50; if instead it is exciting because of the technicality, complexity, environment, and dynamics, then I’ll go for the Sundara. The biggest benefit is that the same album can sound completely through each headphone, so I’ve effectively doubled the amount of music I have! A technical comparison between the two can be found here. . IEMs . For the IEMs, the Moondrop Starfields are a definite improvement on the 1more Triple Drivers, and the treble is much more pleasant to listen to. The mid-range is well presented and picks up the grit of distorted guitars nicely, and the bass has a moderate punch without being overwhelming. Sounds are produced very much inside my head, but as mentioned before, I didn’t want IEMs with a large soundstage to help differentiate external sounds and music when out and about. I do have two complaints, though: . The passive isolation is quite poor, and using them whilst typing, I could clearly hear keystrokes at a similar level to the music (mechanical keyboard, though). Swapping the silicon tips to memory foam (Inairs Air2) helped a bit, but slightly affected the sound. I’ll have to see how well they can handle flying once I’m back to travelling. . | They are a bit fiddly to put on: the tips are quite short so they can fall out easily (foam tips slightly improved this), but the cable goes above and around the ears. On top of playing your music upside down, wrapping the cable around, whilst also holding the tip in can be tricky, especially with long hair. . | One final thing I noticed is that they are very sensitive to low-quality encodings of files; a bad MP3 may sound passable on headphones, but through the Starfields it is unlistenable. So I guess it’s time to finally retire “Linkin_parK-nUmb.exe” and my 240p Youtube rips from back in the day. . Final thoughts . Overall, this was an expensive investestment, but an investment nonetheless, given the amount of use they will get, and it’s one I’m glad I made. . The desktop amp really helped improve my existing headphones and the Sundara provides such a different listening experience, that I now have more options for how I can enjoy the same album. The most expensive item was the desktop DAC, and perhaps I could have saved a bit of money there. Technically it can play 32bit, 384kHz files, but the highest resolution I have is 24bit 96kHz, which was perfectly playable on my old soundcard. The bluetooth is great, but the cheaper Topping D50s also had that. I’ll put it down to future proofing, and the potential for further upgrades by swapping the single-ended RCA cable for a pair of balanced XLR cables. The new IEMs are good, not perfect, but better than a poke in the ear with a sharp frequency response; I’ll still need to see how they handle noisy environments. In terms of value, the FiiO BTR5 is great and I’ve noticed I always look forward to using it; more convenient than the Apple dongle, and it’s able to better drive my headphones. . Well, this turned out to be longer than I intended, but hopefully it’s somewhat useful. As a prize for making it this far, I’ll leave you with the one song for which all headphones should be tested. . Appendices . Volume levels into DACs . A note about audio levels: when connecting to a DAC with a volume controller, the recommendation seems to be to have the volume level of the digital audio as high as possible (or 90% in Windows to avoid distortion), and then adjust the volume level of the DAC output. If then connecting to an amplifier, the DAC output can be maxed, and all the volume adjustment can instead take place on the amplifier. Reducing the volume of digital music equates to a reduction of bit-depth which can reduce quality. . Single-ended versus balanced . Some headphones have the capability to take either a single-ended (SE) input or a balanced input. I won’t go into details, but the balanced output of an amplifier is generally more powerful than the SE output (amplifier specification lists normally show the output power of each output), and a balanced signal is technically subject to less distortion, but this is only noticeable for very long cables. For headphones, it seems that SE is just as good as balanced, provided the SE output is sufficient to drive the headphones. Additionally, balanced cables are very expensive. Amplifiers, too, can take in either SE of balanced signals, provide the DAC has balanced outputs. . Bluetooth versus wired connection . Bluetooth streaming of a music file leads to a possible deterioration in quality, if the original file was of a quality greater than the bluetooth codec can sufficiently encode. Many different codecs exist, with LDAC being the current one which provides the highest quality, when the source drive is configured to stream at maximum quality. Unfortunately, the iPhone only supports AAC, a comparatively lower quality codec, with a transfer rate of 250kbps (about mp3 quality). The advantage of bluetooth for music is the lack of a physical connection: the listener can be more mobile, or proprietary drivers and additional hardware are not required to connect to a device. Since bluetooth is still a digital signal, it can be a way to offload the conversion to analogue to a different device, if the DAC in the source is of lower quality. . Sound leakage from open-back headphones . I had been worried about the amount of sound leakage in the Sundara, especially with the planar magnetic drivers, which produce sound equally in both directions. As a test, I left them playing music at a natural level, and wandered around my apartment. Within the same room, they were very audible, but outside they could barely be heard. Wearing them, there will be reflections off my head from the inside, so they will probably be slightly louder, but my neighbours are so loud anyway I could host a three-day rock festival and still have the moral high ground. .",
            "url": "https://gilesstrong.github.io/website/off-topic/music/2021/10/17/Into-the-world-of-headphone-audio.html",
            "relUrl": "/off-topic/music/2021/10/17/Into-the-world-of-headphone-audio.html",
            "date": " • Oct 17, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Differentiable Programming and MODE",
            "content": ". Earlier this week, I had the pleasure to follow a jam-packed meeting of experts from around the world discussing the application of differential programming to the topic of experiment planning and optimisation. This workshop was organised by the MODE collaboration (a new consortium dedicated to Machine-learning Optimized Design of Experiments, of which yours truly is a member), and kindly hosted by UCLouvain, Belgium. Alongside planning sessions for MODE activities, we heard talks on a range of topics, primarily in the domains of computer science and particle physics. I’ll aim to summarise below, the main discussion points. . What is Differentiable Programming? . In his opening lecture, Atılım Güneş Baydin (MODE, University of Oxford), quoted Differentiable Programming (DP) as a generalisation of Deep Learning (DL); a style of software writing in which lines of code are not explicitly written by the developer, but instead the program is composed of differentiable modules (e.g. neural nets, or other, more simple functions). Through the use of automatic differentiation, these modules can be adjusted such that they fulfil their expected role. . Likely, readers will be familiar with how a convolutional neural network (CNN) can be trained to identify cats, dogs, etc. in images, but consider now a system which instead writes out a text description of the image. . One approach would be to train the CNN to output objects and their positions, and then train a text generator to take these objects and positions and write a summary. However the system is limited by the developer’s choice of CNN output, which forces the image representation to be something traditionally understandable by humans. . The “software 2.0” approach is to forfeit some degree of interpretability and allow the CNN to interpret the image into an information-rich latent space representation, and the text-generator to process this representation into words. Whilst we could pre-train both of these blocks in isolation, the full optimisation will involve training both blocks simultaneously, and in so doing reduce the limitations on the system that would otherwise be present if the blocks were kept separate. . In the first approach we solve our task by creating two artificial subtasks and solving those in isolation. We may be able to solve these subtasks optimally, but that doesn’t guarantee that the complete task will be optimally solved. In the second approach, we specify blocks, connect them up, and solve our task directly. . The figure below shows examples of what the two approaches might achieve: in the first approach, the hypothetical developer did not include any possibility to output texture or overall image colour and this limits the output possibilities of the text generator; whereas in the second approach, embeddings for these details could be learnt via the end-to-end training and do not have to be explicitly included. . Photo credits: Denny Müller . Why MODE? . The workshop began with an introductory talk from Tommaso Dorigo (MODE, INFN-Padova, CMS), in which he lamented that the design of large-scale scientific instruments, such as particle detectors like those found at CERN, was still very much dominated by the immovable beliefs of an old guard of proficient experts, with well-established (but mathematically unproven) design doctrines. . Three identified weaknesses are: . Designs would be created without full consideration of the particle-reconstruction algorithms that will be applied to the detector signals; and with the rise of DL such gaps between design and usage grow even wider. . | Detector design takes place with limited communication between designers and researchers; there is no quick way to measure the impact a design choice has on the end physics goals of the detector. . | Human limitations ⇒ design limitations: difficult to design in 3D, therefore create in 2D and stack modules together; design part of the detector, and mirror it, even though not all physical processes are symmetric; unable to consider all possible designs when trying to keep to a budget. . | . All three of these concerns stem from approaching the problem by splitting the design into subtasks and solving each in isolation. If the aim is to discover di-Higgs production, the detector design process is: design transparent trackers to measure charged particle momenta via magnetic curvature; design dense calorimeters to destroy particles and measure their energy; design sub-detectors to time when particles exit the detector; hope it’s under budget otherwise run studies to see if money can be saved somewhere. Only once a mature design is in place would a few aspects of the design be fine tuned by estimating their effects on the sensitivity to di-Higgs production (see e.g. tables 5.2 and 5.3 of CMS-TDR-020). . As we’ve discussed in the previous section, such isolation of tasks limits optimisation to the study of proxy objectives, which may only be loosely indicative of the desired goal. Dorigo showed in a recent study, that even without relying on DP, by optimising a few parameters of a proposed detector with respect to a relevant metric, he could double the performance of their original design at no extra cost (and as a bonus save O(1e5) euro by replacing some beam-measurement detectors with software). The aim of the MODE collaboration, then, is to learn how to apply this principle of end-goal-oriented optimisation to a larger space of parameters in more complicated scenarios. . How does/can differential programming help . When training a neural network, we specify our goal as a loss function which quantifies how far the output of the network is from the ideal output. We then train the network by feeding in example data, computing the loss based on the output, and then analytically computing the exact effect of every parameter in the network on the performance of the network, and then updating the parameters. . The way in which the effects of the parameters are evaluated is by computing the derivative of the loss with respect to each parameter. Such derivatives can be computed via automatic differentiation (the main subjects of Güneş Baydin’s and Mathieu Blondel’s (Google Brain) talks. Nowadays, this takes place within libraries like PyTorch, TensorFlow, and JAX. Adam Paszke (Google Brain, and developer for PyTorch and JAX (and DEX)) gave a keynote talk on autodiff in JAX. The use of autograd isn’t limited to just optimisation of parameters, but can also be used for uncertainty propagation, as demonstrated by Alberto Ramos Martinez (University of Valencia, CSIC). . This style of optimisation can be extended to modules other than neural networks, provided the system is kept differential, and in an article earlier this year, we, the MODE collaboration, proposed a generic pipeline for detector optimisation, shown below for the case of muon tomography. . . By careful construction of the simulation and inference chains, the final objective function can be differentiated with respect to the detector parameters, which can be updated just like those in a neural network. Some parts of the chain may need to be substituted for surrogate models, either to speed up optimisation, or to make them differentiable. The objective, here, can account for not only the performance (physics sensitivity, imaging accuracy, etc.) but also the budgets required to build the detector (which could be fiscal, heat generation, power consumption, space occupied, etc.). . Whilst other optimisation techniques exist (e.g. genetic algorithms and simulated annealing), a few of the advantages of differential optimisation are that: . The region of optimal solutions can be explored; as discussed in Mikhail Belkin’s (Halicioğlu Data Science Institute, UCSD) keynote talk, large systems are unlikely to converge to a single minima, but rather to a manifold of degenerate minima. By moving along these manifolds, we can sample designs which are expected to perform equally well, but might be more or less feasible to build. Similar to how Fast Geometric Ensembling (1, 2) produces multiple models from a single training. . | Since parameters have physical meaning, and we know analytically how they affect the performance, design interpretation becomes much easier: e.g. “increasing the detector width improves the performance by a measurable rate”. Similarly, unintuitive changes can be identified and examined . | . The rich diagnostic feedback available, and the potential to have multiple designs, means that a framework built around differential optimisation could be invaluable as an assistive tool to detector optimisers. Of course we can never fully simulate all aspects of a detector design to the required detail, nor substitute the expert knowledge of detector designers; our aim is to provide tools to augment their workflow (like predictive text or driver assistance) and to better include notions of the final goal of the experiment. As an example of something similar to this, Auralee Edelen (SLAC) showed in her talk how ML-driven assistance tools could aid experts in tuning their machinery, reducing setup times from ~400 hours/year (~$12M), to about 7 hours/year. . Differential Optimisation in Action . So far this all sounds too good to be true, so the obvious questions are “Does it work? Can it be done?”. . In addition to the talks already mentioned, throughout the workshop we heard examples of how DP is being used for individual parts of experiments (e.g. final stage inference, data simulation, and reconstruction algorithms) in a wide variety of applications: . Topographical imaging and material ID: Volcanic eruption prediction, furnace calibration and damage detection, nuclear waste detection . | Optimal signal inference under systematic uncertainties: INFERNO, Neos . | Particle shower simulation . | Nuclear physics: detector calibration, data preprocessing, learning particle potentials . | Gravitational lensing inference . | Neutrino physics: neutrinoless double-beta decay detection, reconstruction in Cherenkov detectors, track reconstruction and clustering . | . So the technology exists and is being used for reconstruction and inference. But what about in detector optimisation? We heard how IceCube and P-ONE were looking to use automated detector optimisation to help design their next generation of neutrino telescopes, so there’s definitely a demand for it. So how are we progressing on that front? For that, we had two presentations, both from MODE members. . Calorimeter optimisation . Alexey Boldyrev (MODE, Lambda-HSE) spoke on how he and his team were working on optimising the electromagnetic calorimeter (ECal) for the LHCb experiment. Pictured below, the ECal consists of an array of detection modules for measuring the particle showers produced by electromagnetic particles. A range of modules are available, each with varying performance and costs, so with future upgrades planned, the question is “How should a fixed budget be spent to gain maximum performance?”. . . Their approach so far is to simulate the detector responses using Geant4 (an accurate, but non-differentiable detector simulator), reconstruct particles using a boosted decision tree (BDT) regressor (again, non-differentiable), and evaluate a performance metric. This process is relatively slow, and so in order to optimise the detector layout, they fit a Gaussian process model to the evaluated points. This surrogate model can then be used for Bayesian optimisation, in which regions of expected high-performance (exploitation), and regions of unknown performance (exploration) can be evaluated using the full simulation, and used to update the surrogate model. . Whilst this doesn’t yet make use of DP, it does show how automated design can be applied to complicated detectors. Their future plans include approximating the detector response with a differentiable model (GAN), and replacing the BDT with a neural network (again, differentiable), both of which will help move the study towards being able to leverage DP for optimisation. . Muon tomography optimisation . This workshop was a particularly special occasion for me, since I was able to show off a project I, and Dorigo, had been working on: TomOpt - a python framework for providing differential optimisation of detectors for muon tomography, using PyTorch as a backend. This is meant to be a first step into investigating the applicability of the fully differentiable pipeline we outlined in the MODE article (Figure 1). Muon tomography is a useful technique with a wide variety of applications (as was seen in prior presentations), but from an optimisation point of view, is comparatively simple. Given this, and the industry connections and experience within MODE, it seemed a good place to start testing our pipeline. . The aim of muon tomography is to infer characteristics of a volume of space (e.g. check the contents of shipping containers, to identify hazardous materials in sealed barrels, or to indicate area of damage in machinery). Muons are created when cosmic rays interact with the Earth atmosphere, and we can exploit their high penetration power to image volumes; by measuring their incoming and outgoing trajectories using detectors, we can infer what matter they interacted with, and where. The optimisation question is then “How should we configure the detectors to achieve optimal imaging precision for a given budget?”, and this is what TomOpt aims to help with. . The process consists of several stages: . Muons are generated by sampling random functions. . | The muons pass through the incoming detectors, and record hits, which are differentiable with respect to the detector parameters we aim to optimise. . | The muons pass through the hidden volume, and undergo scattering due to the varying density of material. . | The muons pass through the outgoing detectors and record a second set of hits. . | Muon trajectories are fitted to the incoming and outgoing hits, and are also then differentiable w.r.t. the detector parameters. . | We use the trajectories to infer the densities of the hidden volume and pass the predictions to a loss function . | The loss function contains two components: . The error of the predictions (how different the predictions are from the true values); . | The cost of the detectors (computed at the current detector parameters). . | . | This loss can then be differentiated w.r.t. the detector parameters, which can then be updated by gradient descent. . | . . After a few rounds of such an optimisation process, we should arrive at a detector setup which is close to budget and provides good imaging. Currently, though, this is still very much in development, but we’re making progress! . Conclusion . This was the first workshop organised by the MODE collaboration and from the number of attendees (105, of which 39 were speakers), it is clear that there is both great interest and progress in the area of differentiable programming for experiment optimisation. There are even more talks than what I could include here, so I’d recommend checking the agenda. We should soon have recordings of the talks available, too. If any of this work sounds interesting and you are looking to work on similar topics, you may wish to consider contacting MODE to help coordinate efforts. .",
            "url": "https://gilesstrong.github.io/website/differential%20programming/events/mode/optimisation/2021/09/10/Differentiable-Programming-and-MODE.html",
            "relUrl": "/differential%20programming/events/mode/optimisation/2021/09/10/Differentiable-Programming-and-MODE.html",
            "date": " • Sep 10, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Into the INFERNO: parameter estimation under uncertainty - Part 5",
            "content": ". Welcome back to the concluding part of this blog-post series. In case you missed the previous posts: first, second, third, and fourth. . Last time we applied INFERNO to the toy-example presented in the INFERNO paper, and demonstrated how the method made our parameter estimation more resilient to the effects of nuisance parameters that affect both the shapes of input features and the overall normalisation of the classes. Here we&#39;ll look at how we can achieve the same effect without having to know the analytical effect of the nuisance parameters on the input features. . As a reminder, we&#39;re using a PyTorch implementation of INFERNO which I&#39;ve put together. The package can be installed using: . !pip install pytorch_inferno==0.0.1 . Docs are located here and the Github repo is here. I&#39;m installing v0.0.1 here, since it was what I used when writing this post, but for your own work, you should grab the latest version. . As an additional note, these blog posts are fully runnable as Jupyter notebooks. You can either download them, or click the Google Colab badge at the top to follow along! . Approximating INFERNO . The approach as presented last time, requires modelling the effect of the shape nuisances on the inputs in an analytical manner. Usually, though, this is difficult. As an example, in high-energy physics, we often have systematic uncertainties like QCD scale, various energy scales, and correction factors for upstream algorithms. These affect a range of features of the data in complex ways. Additionally, their effects are sometimes modelled by changing fundamental parameters in the Monte Carlo simulator used to generate training data. . Knowing exactly how variations in a given nuisance parameter alter the input features is not normally possible. Indeed, when we have optimised the likelihood in previous posts, we have assumed this and so haven&#39;t fully propagated the effects of the parameters through our classifier, instead we generated up/down systematic variations for the model response and interpolated between them and the nominal template (or extrapolated away from them). The trick, then, is to also do this interpolation/extrapolation inside INFERNO. . When performing a HEP analysis with systematic uncertainties, these up/down variation samples are usually generated via modifying the relevant parameters in the simulation, i.e. for an analysis one has a nominal dataset and also datasets for up and down variations for each nuisance parameter separately. The approximation of INFERNO presented here is designed to work directly on such datasets. . One key point is that we will compare nominal and systematic datasets in terms of template shape, rather than on a per data-point level. This means that the systematic datasets don&#39;t have to match the nominal dataset event-wise, i.e. we don&#39;t need to know how a given event would change if a certain parameter were altered, instead we only need to know the average effect that changing the parameter causes. This certainly simplifies things from both a simulation and implementation point of view. . Abstract implementation . The interpolation-based INFERNO loss in pytorch_inferno is implemented similar to the exact version, i.e. as a callback. Again, there is an abstract class implementing the basic functionality, and the idea is that users can inherit from it to apply to their specific problems, e.g in our case PaperInferno. Note however that even the &quot;abstract&quot; class is still designed for a Poisson counting problem. . Let&#39;s go through the code function by function: . def __init__(self, n:int, true_mu:float, aug_alpha:bool=False, n_alphas:int=0, n_steps:int=100, lr:float=0.1, float_b:bool=False, alpha_aux:Optional[List[Distribution]]=None, b_aux:Optional[Distribution]=None): super().__init__() store_attr() self.true_b = self.n-self.true_mu . The initialisation is similar to the exact version: Asimov count n, and the true signal strength true_mu. The true background count is then computed automatically. n_alphas is the number of shape systematics to expect and float_b is whether or not the background normalisation should be allowed to vary. Any auxiliary measurements for the shape parameters and background yield can be passed as PyTorch distributions to the alpha_aux and b_aux arguments. . There is also the option to augment the nuisance parameters aug_alpha. This is an idea I had which is still experimental - basically the nuisances start each minibatch at random points about their nominal values and undergo Newtonian optimisation according to lr and n_steps. This could perhaps be thought of as a form of regularisation or data-augmentation. In this example it seems to work well, but as I say, it&#39;s still experimental, so I won&#39;t talk about it much here. . def on_train_begin(self) -&gt; None: self.wrapper.loss_func = None for c in self.wrapper.cbs: if hasattr(c, &#39;loss_is_meaned&#39;): c.loss_is_meaned = False . Unlike the exact version, when the training starts we don&#39;t need to cache a tensor of the nuisance parameters, but we do still want to ensure that the model doesn&#39;t have any other losses. . Having passed the nominal input data through the network, we call: . def on_forwards_end(self) -&gt; None: r&#39;&#39;&#39;Compute loss and replace wrapper loss value&#39;&#39;&#39; b = self.wrapper.y.squeeze() == 0 f_s = self.to_shape(self.wrapper.y_pred[~b]) f_b = self.to_shape(self.wrapper.y_pred[b]) f_b_up,f_b_dw = self._get_up_down(self.wrapper.x[b]) self.wrapper.loss_val = self.get_ikk(f_s=f_s, f_b_nom=f_b, f_b_up=f_b_up, f_b_dw=f_b_dw) . This extracts the normalised shapes for signal and background (their PDFs) from the network predictions (self.wrapper.y_pred). Unlike the exact version, the background predictions don&#39;t include the shape nuisances. In order to evaluate these, we pass the background data to self._get_up_down, which passes up/down systematic data through the network and returns the shape templates. Currently it an abstract function to be overridden in a problem-specific class. . @abstractmethod def _get_up_down(self, x:Tensor) -&gt; Tuple[Tensor,Tensor]: r&#39;&#39;&#39;Compute up/down shapes. Override this for specific problem.&#39;&#39;&#39; pass . Now that we have nominal shape templates for signal and background, and the shape templates for background after +1 and -1 sigma variations of each shape-affecting nuisance parameters, we are now at a similar state to when we are evaluating the trained models on the benchmark problems in the previous posts. I.e. we can compute the negative log-likelihood, and optimise it by varying the nuisance parameters based on interpolating between the nominal and ±1-sigma variations, for fixed values of the parameter of interest. . The key point is that we actually start with the nuisances at their nominal values, and we only need to evaluate the parameter of interest at its nominal value, so no optimisation is required. The actual implementation includes code for applying the data augmentation mentioned earlier, but I&#39;ll remove it here for simplicity. . def get_ikk(self, f_s:Tensor, f_b_nom:Tensor, f_b_up:Tensor, f_b_dw:Tensor) -&gt; Tensor: r&#39;&#39;&#39;Compute full hessian at true param values, or at random starting values with Newton updates&#39;&#39;&#39; alpha = torch.zeros((self.n_alphas+1+self.float_b), requires_grad=True, device=self.wrapper.device) with torch.no_grad(): alpha[0] += self.true_mu get_nll = partialler(calc_nll, s_true=self.true_mu, b_true=self.true_b, f_s=f_s, f_b_nom=f_b_nom, f_b_up=f_b_up, f_b_dw=f_b_dw, alpha_aux=self.alpha_aux, b_aux=self.b_aux) nll = get_nll(s_exp=alpha[0], b_exp=self.true_b+alpha[1] if self.float_b else self.true_b, alpha=alpha[1+self.float_b:]) _,h = calc_grad_hesse(nll, alpha, create_graph=True) return torch.inverse(h)[0,0] . So, first we create a tensor, alpha, to contain the parameter of interest and the nuisance parameters (all zero except the parameter of interest (PoI)), and then we compute the NLL using calc_nll (we&#39;ve called this function in the past when evaluating benchmarks, but it has been wrapped by calc_profile to handle multiple PoIs). Once we have the NLL, we can then compute the INFERNO loss as usual by inverting the Hessian of the NLL w.r.t. the PoI and nuisances (calc_grad_hesse), and returning the element corresponding to the PoI. . Problem-specific class . Similar to last time, we want to inherit from our abstract implementation of INFERNO to better suit the problem at hand. PaperInferno is included to work with the toy problem in the original paper. I mentioned at the start that this approximation of INFERNO was designed to work with pre-produced datasets that contain both nominal and systematic datasets, however for this example, we&#39;ll generate the systematic datasets on the fly since we can analytically modify the inputs. If we were to do this properly, we would generate the systematic datasets beforehand, and allow the callback to sample batches from them during training. Probably I&#39;ll eventually add a class to the library that can do this. . Anyway, for now, our class overrides _get_up_down like this: . def _get_up_down(self, x:Tensor) -&gt; Tuple[Tensor,Tensor]: if self.r_mods is None and self.l_mods is None: return None,None u,d = [],[] if self.r_mods is not None: with torch.no_grad(): x = x+self.r_mod_t[0] d.append(self.to_shape(self.wrapper.model(x))) with torch.no_grad(): x = x+self.r_mod_t[1]-self.r_mod_t[0] u.append(self.to_shape(self.wrapper.model(x))) with torch.no_grad(): x = x-self.r_mod_t[1] if self.l_mods is not None: with torch.no_grad(): x = x*self.l_mod_t[0] d.append(self.to_shape(self.wrapper.model(x))) with torch.no_grad(): x = x*self.l_mod_t[1]/self.l_mod_t[0] u.append(self.to_shape(self.wrapper.model(x))) with torch.no_grad(): x = x/self.l_mod_t[1] return torch.stack(u),torch.stack(d) . I.e. it modifies the current data, passes it through the model, and constructs the templates for independent up and down systematics. A proper implementation would instead sample minibatch x from the respective systematic dataset and pass it through the model . DNN training . We train the network in a similar fashion as in part-4, except we now use the approximated version of INFERNO . from pytorch_inferno.utils import init_net from pytorch_inferno.model_wrapper import ModelWrapper from torch import nn def get_model() -&gt; ModelWrapper: net = nn.Sequential(nn.Linear(3,100), nn.ReLU(), nn.Linear(100,100),nn.ReLU(), nn.Linear(100,10), VariableSoftmax(0.1)) init_net(net) # Initialise weights and biases return ModelWrapper(net) . from pytorch_inferno.data import get_paper_data data, test = get_paper_data(n=200000, bs=2000, n_test=1000000) . from pytorch_inferno.inferno_interp import PaperInferno, VariableSoftmax from torch import distributions inferno = PaperInferno(r_mods=(-0.2,0.2), # Let r be a shape nuisance and specify down/up values l_mods=(2.5,3.5), # Let lambda be a shape nuisance and specify down/up values float_b=True, # Let the background yield be a nusiance alpha_aux=[distributions.Normal(0,2), distributions.Normal(0,2)], # Include auxiliary measurements on r &amp; lambda b_aux=distributions.Normal(1000,100)) # Include auxiliary measurements on the background yield . Now let&#39;s train the model. In contrast to the paper, we&#39;ll use ADAM for optimisation, at a slightly higher learning rate, to save time. We&#39;ll also avoid overtraining by saving when the model improves using callbacks. Note that we don&#39;t specify a loss argument. . from fastcore.all import partialler from torch import optim from pytorch_inferno.callback import LossTracker, SaveBest, EarlyStopping model = get_model() model.fit(200, data=data, opt=partialler(optim.Adam,lr=8e-5), loss=None, cbs=[inferno,LossTracker(),SaveBest(&#39;weights/best_inferno.h5&#39;),EarlyStopping(5)]) . &lt;progress value=&#39;103&#39; class=&#39;&#39; max=&#39;200&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 51.50% [103/200 16:23&lt;15:26] &lt;progress value=&#39;50&#39; class=&#39;&#39; max=&#39;50&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [50/50 00:03&lt;00:00] 1: Train=1903.3741125488282 Valid=1099.821317138672 2: Train=949.5193017578125 Valid=871.2384777832032 3: Train=784.0374075317383 Valid=740.7831909179688 4: Train=676.6555770874023 Valid=623.6444390869141 5: Train=588.5027008056641 Valid=579.0961938476562 6: Train=556.9197653198243 Valid=553.1921911621093 7: Train=531.2430877685547 Valid=531.6476770019531 8: Train=511.1146792602539 Valid=512.2304235839844 9: Train=496.7247784423828 Valid=496.2313848876953 10: Train=475.7442514038086 Valid=481.18335876464846 11: Train=465.06764404296877 Valid=467.6013262939453 12: Train=453.10702941894533 Valid=456.12772888183594 13: Train=441.7800866699219 Valid=447.8130987548828 14: Train=433.41732116699217 Valid=437.7292547607422 15: Train=424.49437103271487 Valid=430.0029229736328 16: Train=416.7866101074219 Valid=426.0567614746094 17: Train=412.7526251220703 Valid=419.2637548828125 18: Train=408.7203356933594 Valid=416.4257537841797 19: Train=400.82221466064453 Valid=409.8280517578125 20: Train=399.35216857910154 Valid=407.7358648681641 21: Train=394.8920980834961 Valid=404.62935546875 22: Train=392.22727661132814 Valid=400.5902557373047 23: Train=387.8922058105469 Valid=396.4555090332031 24: Train=386.4134075927734 Valid=395.3793377685547 25: Train=384.1147979736328 Valid=389.80433471679686 26: Train=380.9291650390625 Valid=386.91029174804686 27: Train=377.0340869140625 Valid=388.1162451171875 28: Train=375.92713134765626 Valid=385.6933905029297 29: Train=374.33164367675784 Valid=382.5351153564453 30: Train=371.9804290771484 Valid=380.67197082519533 31: Train=368.66335632324217 Valid=378.97169189453126 32: Train=367.6147705078125 Valid=378.1017291259766 33: Train=363.45591827392576 Valid=377.79085327148437 34: Train=363.84340896606443 Valid=374.8612762451172 35: Train=361.1047927856445 Valid=374.572109375 36: Train=360.6424502563477 Valid=372.1866046142578 37: Train=359.7146035766602 Valid=370.7572198486328 38: Train=358.75559661865236 Valid=371.33667236328125 39: Train=355.9152355957031 Valid=368.7162133789063 40: Train=355.1406524658203 Valid=370.5099066162109 41: Train=352.2504364013672 Valid=367.6858612060547 42: Train=351.0478335571289 Valid=366.42741638183594 43: Train=352.3582815551758 Valid=365.37479431152343 44: Train=349.79847747802734 Valid=364.5373815917969 45: Train=351.1881408691406 Valid=362.6999938964844 46: Train=348.9755392456055 Valid=360.88500427246095 47: Train=347.34831604003904 Valid=360.76511657714843 48: Train=345.6294583129883 Valid=360.39421447753904 49: Train=343.59228576660155 Valid=359.66538818359373 50: Train=346.0175961303711 Valid=357.86841064453125 51: Train=344.6366781616211 Valid=359.06419250488284 52: Train=341.4972285461426 Valid=356.19458435058596 53: Train=341.9684248352051 Valid=356.2013336181641 54: Train=340.62243362426756 Valid=354.7986822509766 55: Train=339.8979489135742 Valid=355.1641271972656 56: Train=340.5874548339844 Valid=354.5460137939453 57: Train=337.78608337402346 Valid=352.9345196533203 58: Train=338.2697674560547 Valid=351.7689556884766 59: Train=336.8008772277832 Valid=352.7039239501953 60: Train=336.6271438598633 Valid=350.5672857666016 61: Train=333.62806350708007 Valid=350.3022137451172 62: Train=333.2799528503418 Valid=351.6690576171875 63: Train=334.6952801513672 Valid=349.2768273925781 64: Train=334.7041619873047 Valid=349.21451782226563 65: Train=333.62857818603516 Valid=351.12249572753905 66: Train=332.74018524169924 Valid=346.973759765625 67: Train=329.8943818664551 Valid=348.11360412597656 68: Train=332.9227757263184 Valid=347.2443560791016 69: Train=329.22062530517576 Valid=346.0074743652344 70: Train=329.7236764526367 Valid=346.23931884765625 71: Train=329.0929122924805 Valid=344.219853515625 72: Train=329.3751695251465 Valid=345.1238238525391 73: Train=329.4808396911621 Valid=345.1286047363281 74: Train=328.91672256469724 Valid=345.9363116455078 75: Train=327.8580014038086 Valid=343.5010699462891 76: Train=327.675544128418 Valid=343.328955078125 77: Train=326.15745666503904 Valid=341.92960876464844 78: Train=328.9135433959961 Valid=343.7149389648437 79: Train=326.5412274169922 Valid=340.72755249023436 80: Train=327.1108027648926 Valid=342.9395086669922 81: Train=325.51490631103513 Valid=342.25386932373044 82: Train=323.1186293029785 Valid=339.41467224121095 83: Train=323.5835791015625 Valid=340.5097528076172 84: Train=323.02451416015623 Valid=340.3460504150391 85: Train=323.5133598327637 Valid=339.0770629882812 86: Train=324.4291862487793 Valid=342.1396295166016 87: Train=322.38175201416016 Valid=341.73817932128907 88: Train=321.80700439453125 Valid=340.46791748046877 89: Train=323.08181732177735 Valid=336.96313232421875 90: Train=322.5729107666016 Valid=338.9583264160156 91: Train=321.12098159790037 Valid=340.28571838378906 92: Train=322.7699349975586 Valid=337.8809533691406 93: Train=320.90445220947265 Valid=336.4360284423828 94: Train=321.4063262939453 Valid=336.58940979003904 95: Train=319.529539642334 Valid=337.05964599609376 96: Train=321.07305908203125 Valid=334.4169158935547 97: Train=319.94526412963864 Valid=337.0082684326172 98: Train=320.0385334777832 Valid=335.60974365234375 99: Train=318.8555616760254 Valid=333.65868713378904 100: Train=317.8938897705078 Valid=336.39325256347655 101: Train=318.79115554809573 Valid=334.6770684814453 102: Train=317.1104347229004 Valid=334.6987811279297 103: Train=316.3874523925781 Valid=334.5063348388672 104: Train=317.27213409423825 Valid=335.46395202636717 Early stopping Loading best model with loss 333.65868713378904 . Having trained the model, we again want to bin the predictions via hard assignments . from pytorch_inferno.inferno import InfernoPred preds = model._predict_dl(test, pred_cb=InfernoPred()) . &lt;progress value=&#39;250&#39; class=&#39;&#39; max=&#39;250&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [250/250 00:12&lt;00:00] import pandas as pd df = pd.DataFrame({&#39;pred&#39;:preds.squeeze()}) df[&#39;gen_target&#39;] = test.dataset.y df.head() . pred gen_target . 0 9 | 1.0 | . 1 9 | 1.0 | . 2 6 | 1.0 | . 3 4 | 1.0 | . 4 9 | 1.0 | . from pytorch_inferno.plotting import plot_preds import numpy as np plot_preds(df, bin_edges=np.linspace(0,10,11)) . Parameter estimation . To save time let&#39;s test the method on just the easiest and most difficult benchmarks. For readability, I&#39;ll hide the code cells (they&#39;ll still show up if you open the notebook yourself to run it). . Benchmark 0 - No nuisance parameters . from pytorch_inferno.plotting import plot_likelihood plot_likelihood(nll-nll.min()) . As a reminder, the binary cross-entropy classifier achieved a width of 15.02, and the exact version of INFERNO got 15.86, so the interpolation version does quite well! . Benchmark 4 - Normalisation uncertainty with auxiliary measurements . %%capture --no-display alpha_aux = [torch.distributions.Normal(0,2), torch.distributions.Normal(0,2)] nll = profiler(alpha_aux=alpha_aux, float_b=True, b_aux=torch.distributions.Normal(1000,100), **b_shapes).cpu().detach() plot_likelihood(nll-nll.min()) . &lt;progress value=&#39;61&#39; class=&#39;&#39; max=&#39;61&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [61/61 00:18&lt;00:00] Again, the interpolation method does pretty well: the BCE classifier blew up to a width of 27.74, and the exact version of INFERNO got a much better constraint of 19.08 (slightly high compared to the paper value of 18.79). . Closing . In this post we&#39;ve looked at how the INFERNO method can be implemented without requiring analytical knowledge of how the nuisance parameters affect the inputs in the data. Instead we have approximated their affects by interpolating between up/down systematics datasets about the the nominal values. Such datasets are already produced during the course of a typical HEP analysis, and so this approximation improves the potential to be a drop-in replacement for the event-level classifier used in contemporary analyses. . Whilst I&#39;ve only tested the approximation using the same toy-data problem used by the paper, I have found that it was able to match the performance of the exact version, so I&#39;ll be interested to see how well it performs on real-world problems. Certainly, given the difference in performance illustrated by INFERNO (exact or interpolated) and a traditional BCE network, I think the method has a lot to offer in domains where uncertainties play a key role, and I&#39;m looking forward to testing it out. .",
            "url": "https://gilesstrong.github.io/website/statistics/hep/inferno/2021/02/02/inferno-5.html",
            "relUrl": "/statistics/hep/inferno/2021/02/02/inferno-5.html",
            "date": " • Feb 2, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Into the INFERNO: parameter estimation under uncertainty - Part 4",
            "content": ". Welcome back to the fourth part of this blog-post series. In case you missed the previous posts: first, second, and third. This post will continue on from part-3, so I&#39;d recommend re-reading that first, anyway. . Last time we worked through the toy-example presented in the INFERNO paper, and demonstrated how the typical approach in HEP would go about solving it. We then went on to see how the presence of nuisance parameters in the feature distributions or yield normalisations can really spoil the precision of our measurement. Now let&#39;s use INFERNO to tackle the same problem! . As a reminder, we&#39;re using a PyTorch implementation of INFERNO which I&#39;ve put together. The package can be installed using: . !pip install pytorch_inferno==0.0.1 . Docs are located here and the Github repo is here. I&#39;m installing v0.0.1 here, since it was what I used when writing this post, but for your own work, you should grab the latest version. . As an additional note, these blog posts are fully runnable as Jupyter notebooks. You can either download them, or click the Google Colab badge at the top to follow along! . Recap . Put simply, INFERNO aims to provide a loss function for the network that fully captures the nature of inference problem and directly optimises the network according to the final metric; the precision of the parameter estimate. First, though, let&#39;s recap our approach from the last post to remind ourselves what this involves: First we trained a binary classifier for signal and background, mapping the 3 input features down to a single class prediction in which the two classes display high linear separation. . In the presence of no nuisance parameters: . We binned the predictions and normalised to get PDFs of the signal ($f_s$) and background ($f_b$) | The PDFs were then renormalised according to: Expected the signal and background yields: the Asimov dataset $N_{a,i}=50 cdot f_{s,i}+1000 cdot f_{b,i}$ | The expected background and a range of signal strengths $N_{ mu,i}= mu cdot f_{s,i}+1000 cdot f_{b,i}$ | | The negative log-likelihood of the Poisson model computed at the Asimov count was evaluated for the range of signal strengths $NLL( mu)=- sum_i left[ ln left[ frac{N_{ mu,i}^{N_{a,i}}}{N_{a,i}!}e^{N_{ mu,i}} right] right]$ | The uncertainty on the signal strength $ mu$ was computed as half the width of the NLL scan at the points 0.5 greater than the minimum value (the 1-sigma interval) | In the presence of systematics: . 1-sigma up/down shifts in the input features were computed | The DNN predictions for shifted inputs were computed, along with the nominal shapes normalised and binned to get PDFs of the signal ($f_s$) and background ($f_{b, theta}$) | The PDFs were then renormalised according to: Expected the signal and background yields: the Asimov dataset $N_{a,i}=50 cdot f_{s,i}+1000 cdot f_{b,i}$ | The expected background and a range of signal strengths and an uncertain background $N_{ mu, theta,n_b,i}= mu cdot f_{s,i}+n_b cdot f_{b, theta,i}$ | | The negative log-likelihood of the Poisson model computed at the Asimov count was evaluated for the range of signal strengths $NLL( mu, theta,n_b)=- sum_i left[ ln left[ frac{N_{ mu, theta,n_b,i}^{N_{a,i}}}{N_{a,i}!}e^{N_{ mu, theta,n_b,i}} right] right]$ | The NLL values were then adjusted according to any auxiliary measurements on the nuisances, e.g. $NLL rightarrow NLL-C$, where $C$ could be e.g. a log-normal PDF evaluated at the proposed $ theta$ value | The NLL values were then minimised separately for each value of $ mu$ by optimising the nuisance parameters though Newtonian updates based on the first- and second-order derivatives of the NLL w.r.t. to the nuisances. (Nuisances affecting the shape of the inputs features were interpolated between the up/down 1-sigma template shifts). | The uncertainty on the signal strength $ mu$ was computed as half the width of the optimised NLL scan at the points 0.5 greater than the minimum value (the 1-sigma interval) | The INFERNO loss . INFERNO effectively performs the entire loop above, but with a few simplifications to help speed it up. In order to function as a loss, we need the final value to be a single number which can be differentiated w.r.t. the network parameters. The loss also needs to represent the width of the NLL scan, and take into account the effect of nuisance parameters. . As said, we want to quantify the width of the NLL scan without having to compute the full scan and optimise the nuisance parameters. Instead, let&#39;s compute the NLL at the nominal values of the parameters and consider this point in the parameter space (i.e. the NLL surface projected in $( mu, theta)$ space). Since we computed the NLL at the nominal parameter values (in our example $ mu=50$, $n_b$=1000, and $ bar{ theta}= bar{0}$), we are therefore at the minimum, and $ nabla NLL=0$. However the second-order derivatives will not be zero. $ partial^2_ mu NLL$ will characterise how quickly the NLL surface becomes steep along the $ mu$ direction. The greater that $ partial^2_ mu NLL$ is, the thinner the width at $0.5+ min NLL$, and so the precision on $ mu$ increases. Similarly, $ partial^2_ theta NLL$ will characterise the effect of the nuisances on the NLL surface. . Computing the full Hessian of the NLL w.r.t. the parameter of interest and the nuisances we get a matrix of the form: . $$I= begin{bmatrix} partial_{ mu mu} NLL &amp; partial_{ mu n_b} NLL &amp; partial_{ mu theta} NLL partial_{n_b mu} NLL &amp; partial_{n_b n_b} NLL &amp; partial_{n_b theta} NLL partial_{ theta mu} NLL &amp; partial_{ theta n_b} NLL &amp; partial_{ theta theta} NLL end{bmatrix}$$This is referred to as the Fisher information matrix $I$. . The diagonal elements charaterise the expected variance of each parameter, and the off-diagonal elements characterise the interactions between the parameters. Inverting the Hessian and then taking the $ mu mu$ element provides us with a single value whose minimisation will lead to a narrowing to the NLL width in $ mu$, whilst accounting for the interactions between $ mu$ and the nuisance parameters. Note that this is not the same as taking the reciprocal of the $ mu mu$ element of the Hessian, since that ignores the effect of the nuisance parameters. . Our loss is therefore $(I^{-1})_{0,0}$ (where the parameter of interest, $ mu$, is the zeroth element) . Soft binning . One problem, though, is that our usual approach to computing NLL relies on binning the output of the network. Such an approach here would not allow the loss to be differentiable. . The approach taken by INFERNO is to mimic the binning by having multiple outputs for the DNN. A softmax output activation function is then used to normalise the sum of outputs per datapoint to be one, effectively providing a soft-assignment of the datapoints to each of the &#39;bins&#39;. The inputs to the softmax can be multiplicatively rescaled to better approximate a hard assignment to a given bin, whilst still allowing the loss to be differentiated. . Note that in this case, in might be more intuitive to think of each output of the DNN not as a bin in a histogram, but as an abstract class (or cluster) of datapoints which emerge as a function of the training: certainly there is no reason to expect the outputs to exhibit any form of ordering, like a histogram might, and due to the random initialisation of the network parameters, no two trainings will result in the same output densities. . One could expect, though, that the learned clustering would instead follow differing densities of signal and background based on confidence in assignment (e.g. strongly expect signal, strongly expect background, and reasonably signal-like), the stringency of which varies according to the yields of signal and background, and any uncertainties on them. Effectively, the network is allowed to learn its own binning of the data for the particular inference task at hand. . INFERNO implementation . Variable Softmax for binning . The softmax activation performs the following operation: . $$ sigma( bar{x})_i = frac{e^{x_i}}{ sum_j e^{x_j}}$$ . i.e. it normalises each output of the network $x_i$ according to the all outputs $ bar{x}$, such that $ sum_i sigma( bar{x})_i=1$ . As an example: . from torch import Tensor, nn nn.Softmax(dim=-1)(Tensor([1,2,3])) . tensor([0.0900, 0.2447, 0.6652]) . The output vector gets normalised to one, with each input element having a proportionate representation in the output. Since we are using the softmax to simulate the hard assignment of predictions to bins, however, we want to magnify any slight differences in the predictions, such that the largest input to the softmax has a greater than proportional representation in the output. This can be achieved by rescaling the inputs prior to the softmax (VariableSoftmax): . class VariableSoftmax(nn.Softmax): r&#39;&#39;&#39;Softmax with temperature&#39;&#39;&#39; def __init__(self, temp:float=1, dim:int=-1): super().__init__(dim=dim) self.temp = temp def forward(self, x:Tensor) -&gt; Tensor: return super().forward(x/self.temp) . from pytorch_inferno.inferno import VariableSoftmax VariableSoftmax(0.1)(Tensor([1,2,3])) . tensor([2.0611e-09, 4.5398e-05, 9.9995e-01]) . Now the first two elements have basically no representation, and the entire datapoint is effectively assigned to the last &#39;bin&#39;. N.B. This rescaling could potentially be learnt by the network during training, however by setting the rescaling ourselves, we still allow the network to learn an optimal rescaling, but in a way which keeps the output of the last linear layer close to a standard deviation of one. . Loss-DNN class interaction . Implementation of the INFERNO loss requires a slightly more complex interaction between the loss and the DNN, than other, more basic, losses. This potentially makes it difficult to integrate into a traditional training configuration which expects to just pass predictions to the loss function and get back the loss value. . The training setup included in pytorch_inferno includes an extended integration of callbacks (inspired by fastai). Let&#39;s look at the fitting function code included in the ModelWrapper class: . def fit(self, n_epochs:int, data:DataPair, opt:Callable[[Generator],optim.Optimizer], loss:Optional[Callable[[Tensor,Tensor],Tensor]], cbs:Optional[Union[AbsCallback,List[AbsCallback]]]=None) -&gt; None: def fit_epoch(epoch:int) -&gt; None: self.model.train() self.state = &#39;train&#39; self.epoch = epoch for c in self.cbs: c.on_epoch_begin() for b in progress_bar(self.data.trn_dl, parent=self.mb): self._fit_batch(*b) for c in self.cbs: c.on_epoch_end() self.model.eval() self.state = &#39;valid&#39; for c in self.cbs: c.on_epoch_begin() for b in progress_bar(self.data.val_dl, parent=self.mb): self._fit_batch(*b) for c in self.cbs: c.on_epoch_end() if cbs is None: cbs = [] elif not is_listy(cbs): cbs = [cbs] self.cbs,self.stop,self.n_epochs = cbs,False,n_epochs self.data,self.loss_func,self.opt = data,loss,opt(self.model.parameters()) for c in self.cbs: c.set_wrapper(self) for c in self.cbs: c.on_train_begin() self.mb = master_bar(range(self.n_epochs)) for e in self.mb: fit_epoch(e) if self.stop: break for c in self.cbs: c.on_train_end() . and the _fit_batch function: . def _fit_batch(self, x:Tensor, y:Tensor, w:Tensor) -&gt; None: self.x,self.y,self.w = to_device(x,self.device),to_device(y,self.device),to_device(w,self.device) for c in self.cbs: c.on_batch_begin() self.y_pred = self.model(self.x) if self.state != &#39;test&#39; and self.loss_func is not None: self.loss_func.weights = self.w self.loss_val = self.loss_func(self.y_pred, self.y) for c in self.cbs: c.on_forwards_end() if self.state != &#39;train&#39;: return self.opt.zero_grad() for c in self.cbs: c.on_backwards_begin() self.loss_val.backward() for c in self.cbs: c.on_backwards_end() self.opt.step() for c in self.cbs: c.on_batch_end() . Throughout the training loop, there are lines of the form for c in self.cbs: c.on_batch_end(). These allow the callbacks to act at appropriate times during the training. . Three important things to note are: . for c in self.cbs: c.set_wrapper(self) means that every callback class has direct access to the DNN (and any other callbacks) | self.x,self.y,self.w = to_device(x,self.device),to_device(y,self.device),to_device(w,self.device) means that the data for the minibatch are properties of the model, and so can be accessed and affected by the callbacks | if self.state != &#39;test&#39; and self.loss_func is not None: means we can set the loss function to None, and manually set the value of self.loss_val within a callback. | The INFERNO loss in pytorch_inferno is therefore implemented as a callback and so gains access to an extra level of integration with the training loop. There is an abstract class implementing the basic functionality, and the idea is that users can inherit from it to apply to their specific problems, e.g in our case PaperInferno. Note however that even the &quot;abstract&quot; class is still designed for a Poisson counting problem. . Abstract implementation . Let&#39;s go through the code function by function: . class AbsInferno(AbsCallback): def __init__(self, n:int, true_mu:float, n_alphas:int=0, float_b:bool=False, alpha_aux:Optional[List[Distribution]]=None, b_aux:Optional[Distribution]=None): super().__init__() store_attr() self.true_b = self.n-self.true_mu . We initialise the class with the Asimov count n, and the true signal strength true_mu. The true background count is then computed automatically. n_alphas is the number of shape systematics to expect and float_b is whether or not the background normalisation should be allowed to vary. Any auxiliary measurements for the shape parameters and background yield can be passed as PyTorch distributions to the alpha_aux and b_aux arguments. . def on_train_begin(self) -&gt; None: self.wrapper.loss_func = None for c in self.wrapper.cbs: if hasattr(c, &#39;loss_is_meaned&#39;): c.loss_is_meaned = False self.alpha = torch.zeros((self.n_alphas+1+self.float_b), requires_grad=True, device=self.wrapper.device) with torch.no_grad(): self.alpha[0] = self.true_mu . This is called whenever the training starts. It ensures that the model doesn&#39;t have a loss function already, and ensures that any other callbacks will correctly average the loss for early-stopping, model saving, and real-time feedback. Finally, we cache a tensor for the nuisance parameters. The zeroth element will be our parameter of interest, which we set to the true value in way that doesn&#39;t affect the stored gradient. . def on_batch_begin(self) -&gt; None: self.b_mask = self.wrapper.y.squeeze() == 0 self.aug_data(self.wrapper.x) . This is called just before a minibatch is passed through the network. self.wrapper.y are the true target values, and so self.b_mask is a Boolean mask of all the background datapoints (we cache it now, since it is used at several points). self.aug_data is an abstract method, which must be overriden in the problem-specific inheriting class. This is a key function since it allows the effect of shape-nuisances to have derivatives w.r.t. the network parameters. Let&#39;s quickly jump to see how it is implemented for the INFERNO paper problem: . def aug_data(self, x:Tensor) -&gt; None: if self.float_r: x[self.b_mask,0] += self.alpha[1+self.float_b] # If float_b, alpha_r is element 2 not 1 if self.float_l: x[self.b_mask,2] *= (self.alpha[-1]+self.l_init)/self.l_init . As discussed in part-3 , due to the construction of the problem, we know analytically the exact effects of our two shape nuisances on the input data: a shift in $r$ causes a linear shift in zeroth input feature values, and a change in $ lambda$ leads to a rescaling of the second input feature. x is the incoming minibatch, and we use the cached background mask to only affect the background data. . Note that elements of self.alpha that are indexed are all zero: therefore aug_data data never actually modifies the values of the input data, instead we are adding &#39;the potential to be modified by the nuisance parameters&#39;, which allows the NLL to be differentiated w.r.t. the shape nuisances without having interpolate the shapes from up/down 1-sigma templates. Of course, this relies on knowing the analytical (or well modelled) effect of the nuisances in the input features, and in part 5 we&#39;ll look at a way around this. . Having passed the input data through the network, we call: . def on_forwards_end(self) -&gt; None: r&#39;&#39;&#39;Compute loss and replace wrapper loss value&#39;&#39;&#39; def to_shape(p:Tensor) -&gt; Tensor: f = p.sum(0)+1e-7 return f/f.sum() f_s = to_shape(self.wrapper.y_pred[~self.b_mask]) f_b = to_shape(self.wrapper.y_pred[self.b_mask]) f_b_asimov = to_shape(self.wrapper.model(self.wrapper.x[self.b_mask].detach())) if len(self.alpha) &gt; 1 else f_b self.wrapper.loss_val = self.get_inv_ikk(f_s=f_s, f_b=f_b, f_b_asimov=f_b_asimov) . This extracts the normalised shapes for signal and background (their PDFs) from the network predictions (self.wrapper.y_pred). The background predictions will include the shape nuisances, so in order to get the Asimov shape for background (which shouldn&#39;t be affected by nuisances), we need to detach the background inputs (removes the nuisance parameters) and pass it through the network again. Finally, we compute the loss (detailed below) and set the loss value of the model manually. . def get_inv_ikk(self, f_s:Tensor, f_b:Tensor, f_b_asimov:Tensor) -&gt; Tensor: r&#39;&#39;&#39;Compute full hessian at true param values&#39;&#39;&#39; b_exp = self.true_b+self.alpha[1] if self.float_b else self.true_b t_exp = (self.alpha[0]*f_s)+(b_exp*f_b) asimov = (self.true_mu*f_s)+(self.true_b*f_b_asimov) nll = -torch.distributions.Poisson(t_exp).log_prob(asimov).sum() if self.alpha_aux is not None: # Constrain shape nuisances if len(self.alpha_aux) != self.n_alphas: raise ValueError(&quot;Number of auxillary measurements must match the number of nuisance parameters&quot;) for a,x in zip(self.alpha[1+self.float_b:], self.alpha_aux): # Constrain shapes if x is not None: nll = nll-x.log_prob(a) if self.b_aux is not None: nll = nll-self.b_aux.log_prob(b_exp) _,h = calc_grad_hesse(nll, self.alpha, create_graph=True) return torch.inverse(h)[0,0] . Here we compute the Fisher information matrix of the negative log-likelihood, invert it, and return the element corresponding to our parameter of interest. If the background yield is uncertain, then the &#39;potential to be modified&#39; is added using the alpha tensor (self.alpha[1] is zero, but the tensor tracks gradients). Next we compute the expected and Asimov yields per bin t_exp and asimov. These will have the same values, the difference is that t_exp can be affected by changes in the nuisance parameters, alpha, but asimov cannot. We then compute the NLL as normal, and add any constraints due to auxiliary measurements. calc_grad_hesse returns the Hessian (Fisher information matrix), which we then invert and return the $ mu mu$ element of. . DNN training . We train the network in a similar fashion as in part-3, except we want to increase the batch size since we need to have an accurate representation of the bin populations to prevent statisitical fluctuations in the loss. The paper uses a batch size of 2000, 10 outputs, and a softmax temperature of 0.1. . from pytorch_inferno.utils import init_net from pytorch_inferno.model_wrapper import ModelWrapper def get_model() -&gt; ModelWrapper: net = nn.Sequential(nn.Linear(3,100), nn.ReLU(), nn.Linear(100,100),nn.ReLU(), nn.Linear(100,10), VariableSoftmax(0.1)) init_net(net) # Initialise weights and biases return ModelWrapper(net) . from pytorch_inferno.data import get_paper_data data, test = get_paper_data(n=200000, bs=2000, n_test=1000000) . As mentioned above, we need to specify a concrete implementation of INFERNO for our specific problem. pytorch_inferno already includes a class for the paper example: PaperInferno. . class PaperInferno(AbsInferno): r&#39;&#39;&#39;Inheriting class for dealing with INFERNO paper synthetic problem&#39;&#39;&#39; def __init__(self, float_r:bool, float_l:bool, l_init:float=3, n:int=1050, true_mu:int=50, float_b:bool=False, alpha_aux:Optional[List[Distribution]]=None, b_aux:Optional[Distribution]=None): super().__init__(n=n, true_mu=true_mu, n_alphas=float_r+float_l, float_b=float_b, alpha_aux=alpha_aux, b_aux=b_aux) self.float_r,self.float_l,self.l_init = float_r,float_l,l_init def aug_data(self, x:Tensor) -&gt; None: if self.float_r: x[self.b_mask,0] += self.alpha[1+self.float_b] # If float_b, alpha_r is element 2 not 1 if self.float_l: x[self.b_mask,2] *= (self.alpha[-1]+self.l_init)/self.l_init . This contains options for floating different parameters of the model and allows us to define specific losses for each of the benchmark problems in the paper. In part-3, we evaluated our cross-entropy DNN against benchmarks 0, 2, 3, and 4. The paper results (table 2), however show that a model trained using INFERNO specified for benchmark 4 performs nearly as well on other benchmark problems as models trained specifically for that benchmark. To save time, then we&#39;ll just train for benchmark 4, now, but you can run this notebook yourself to check out the other trainings. . from pytorch_inferno.inferno import PaperInferno from torch import distributions inferno = PaperInferno(float_l=True, # Let lambda be a shape nuisance float_r=True, # Let r be a shape nuisance float_b=True, # Let the background yield be a nusiance alpha_aux=[distributions.Normal(0,2), distributions.Normal(0,2)], # Include auxiliary measurements on r &amp; lambda b_aux=distributions.Normal(1000,100)) # Include auxiliary measurements on the background yield . Now let&#39;s train the model. In contrast to the paper, we&#39;ll use ADAM for optimisation, at a slightly higher learning rate, to save time. We&#39;ll also avoid overtraining by saving when the model improves using callbacks. Note that we don&#39;t specify a loss argument. . from fastcore.all import partialler from torch import optim from pytorch_inferno.callback import LossTracker, SaveBest, EarlyStopping model = get_model() model.fit(200, data=data, opt=partialler(optim.Adam,lr=8e-5), loss=None, cbs=[inferno,LossTracker(),SaveBest(&#39;weights/best_inferno.h5&#39;),EarlyStopping(5)]) . &lt;progress value=&#39;58&#39; class=&#39;&#39; max=&#39;200&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 29.00% [58/200 10:36&lt;25:58] &lt;progress value=&#39;50&#39; class=&#39;&#39; max=&#39;50&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [50/50 00:03&lt;00:00] 1: Train=3912.3916760253906 Valid=2806.8546923828126 2: Train=1746.002896118164 Valid=1328.9496655273438 3: Train=1223.951845703125 Valid=1056.1231018066405 4: Train=916.6781005859375 Valid=825.12619140625 5: Train=828.3240393066407 Valid=756.0421423339844 6: Train=786.2163452148437 Valid=720.3921081542969 7: Train=730.0841842651367 Valid=695.8928698730468 8: Train=707.7799942016602 Valid=668.0408581542969 9: Train=672.0432852172852 Valid=643.1469519042969 10: Train=652.324967956543 Valid=623.7019995117188 11: Train=630.8129745483399 Valid=625.9892742919922 12: Train=604.371823425293 Valid=592.8834387207031 13: Train=596.728298034668 Valid=587.4838079833985 14: Train=584.1115322875977 Valid=579.2229278564453 15: Train=581.4167013549804 Valid=555.0884375 16: Train=559.1637445068359 Valid=584.8051611328125 17: Train=555.4294171142578 Valid=539.6500164794921 18: Train=558.395673828125 Valid=527.4596801757813 19: Train=531.9894320678711 Valid=527.0246984863281 20: Train=528.8188244628906 Valid=515.791996459961 21: Train=517.4443655395507 Valid=516.7953771972657 22: Train=510.71862731933595 Valid=504.1253973388672 23: Train=507.957060546875 Valid=498.80123107910157 24: Train=494.1633874511719 Valid=489.3322357177734 25: Train=483.1570764160156 Valid=485.0844805908203 26: Train=482.27189544677736 Valid=478.40015991210936 27: Train=477.8729238891602 Valid=472.3101361083984 28: Train=470.5329180908203 Valid=470.3123968505859 29: Train=468.2646893310547 Valid=457.8105181884766 30: Train=458.0747915649414 Valid=462.7433447265625 31: Train=458.2151498413086 Valid=450.1757476806641 32: Train=446.45051239013674 Valid=443.7071661376953 33: Train=434.5655450439453 Valid=431.2003662109375 34: Train=421.26378143310546 Valid=432.00623901367186 35: Train=423.9385678100586 Valid=422.3200036621094 36: Train=411.33883026123044 Valid=421.1802227783203 37: Train=413.88484771728514 Valid=412.0554864501953 38: Train=407.59821685791013 Valid=415.3173254394531 39: Train=410.2136459350586 Valid=418.28903625488283 40: Train=401.4928482055664 Valid=403.48400024414065 41: Train=399.75360595703125 Valid=402.41387817382815 42: Train=394.4216546630859 Valid=394.7416613769531 43: Train=391.969521484375 Valid=393.3900451660156 44: Train=390.40508911132815 Valid=390.5017736816406 45: Train=384.7444723510742 Valid=391.31961853027343 46: Train=381.36308807373047 Valid=387.21651123046877 47: Train=379.6128295898437 Valid=390.54322143554685 48: Train=381.25733673095704 Valid=377.8986755371094 49: Train=378.2297705078125 Valid=377.49916015625 50: Train=376.3330563354492 Valid=377.2391516113281 51: Train=372.25685943603514 Valid=379.7720831298828 52: Train=370.8914962768555 Valid=375.6806671142578 53: Train=369.37719482421875 Valid=378.1207989501953 54: Train=371.9230255126953 Valid=372.4212335205078 55: Train=368.7582391357422 Valid=378.5429351806641 56: Train=373.219296875 Valid=376.8352526855469 57: Train=369.425950012207 Valid=378.44209838867187 58: Train=369.88836273193357 Valid=375.1474053955078 59: Train=373.11502960205075 Valid=375.3876696777344 Early stopping Loading best model with loss 372.4212335205078 . Having trained the model, we can now check the predictions on the test set, however the model currently outputs probabilities for each class and datapoint, but our inference setup requires a single number per datapoint (the bin index), so we want to perform a hard assignment of data to bins via an $ arg max$ operation. The prediction methods of the ModelWrapper allow us to pass a callback to affect to the predictions of the model without us having to process them manually ourselves. InfernoPred will perform the hard assignment, turning the predictions into single numbers. . class InfernoPred(PredHandler): r&#39;&#39;&#39;Prediction handler for hard assignments&#39;&#39;&#39; def get_preds(self) -&gt; np.ndarray: return np.argmax(self.preds, 1) . Note that potentially we could stick with a soft assignment, in which datapoints are split across several bins by different amounts, however generally in HEP people try to avoid messing with the real collider data, so we&#39;ll stick with hard assignments for now. . from pytorch_inferno.inferno import InfernoPred preds = model._predict_dl(test, pred_cb=InfernoPred()) . &lt;progress value=&#39;250&#39; class=&#39;&#39; max=&#39;250&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [250/250 00:09&lt;00:00] import pandas as pd df = pd.DataFrame({&#39;pred&#39;:preds.squeeze()}) df[&#39;gen_target&#39;] = test.dataset.y df.head() . pred gen_target . 0 | 0 | 1.0 | . 1 | 3 | 1.0 | . 2 | 6 | 1.0 | . 3 | 1 | 1.0 | . 4 | 1 | 1.0 | . from pytorch_inferno.plotting import plot_preds import numpy as np plot_preds(df, bin_edges=np.linspace(0,10,11)) . From the above plot, we can see that as expected, the bins don&#39;t display any form of ordering, but rather varying densities of class population. One possible interpretation might be that the DNN found it optimal to have a few bins with a strong presence of background, and instead split the signal over a range of bins, each with weaker amounts of background . Parameter estimation . Now we can run the benchmark tests as before. The predictions are already binned, we just need to rename the column to allow it to work with the inference code . df[&#39;pred_bin&#39;] = df.pred . from pytorch_inferno.inference import get_shape f_s,f_b = get_shape(df,targ=1),get_shape(df,targ=0) # Gets normalised shape for each class f_s,f_b . (tensor([0.1145, 0.2698, 0.0076, 0.2550, 0.0169, 0.1566, 0.0922, 0.0237, 0.0433, 0.0204]), tensor([0.4742, 0.0227, 0.0024, 0.0498, 0.0028, 0.3600, 0.0153, 0.0046, 0.0071, 0.0611])) . Benchmark 0 - No nuisance parameters . asimov = (50*f_s)+(1000*f_b) asimov, asimov.sum() . (tensor([479.8850, 36.1540, 2.8191, 62.5361, 3.6305, 367.8640, 19.9573, 5.8051, 9.2734, 62.0755]), tensor(1049.9999)) . Now we can compute the negative loglikelihood for a range of $ mu$ values, as usual . import torch n = 1050 mu = torch.linspace(20,80,61) nll = -torch.distributions.Poisson((mu[:,None]*f_s)+(1000*f_b)).log_prob(asimov).sum(1) nll . tensor([27.9274, 27.7851, 27.6486, 27.5183, 27.3931, 27.2736, 27.1600, 27.0515, 26.9484, 26.8504, 26.7580, 26.6700, 26.5877, 26.5098, 26.4373, 26.3694, 26.3065, 26.2478, 26.1947, 26.1454, 26.1011, 26.0611, 26.0255, 25.9940, 25.9670, 25.9448, 25.9262, 25.9124, 25.9019, 25.8958, 25.8939, 25.8959, 25.9020, 25.9116, 25.9252, 25.9425, 25.9639, 25.9890, 26.0177, 26.0498, 26.0854, 26.1249, 26.1679, 26.2143, 26.2641, 26.3176, 26.3740, 26.4338, 26.4972, 26.5637, 26.6334, 26.7066, 26.7826, 26.8620, 26.9445, 27.0302, 27.1184, 27.2098, 27.3043, 27.4023, 27.5027]) . from pytorch_inferno.plotting import plot_likelihood plot_likelihood(nll-nll.min()) . In part-3 our cross-entropy classifier achieved a width of 15.02, so INFERNO performs just slightly worse when no nuisances are present, however that&#39;s still not bad considering that the DNN was trained to expect nuisances to be present. . Nuisance parameters . Shape variations . As before, we need to compute all the shifts in the background PDF. Following the paper, $r$ is shifted by $ pm0.2$, and $ lambda$ by $ pm0.5$. Remember that these shifts only affect the background process. get_paper_syst_shapes will shift the data, recompute the new model predictions, and then compute the new PDF for background. . from pytorch_inferno.inference import get_paper_syst_shapes bkg = test.dataset.x[test.dataset.y.squeeze() == 0] # Select the background test data b_shapes = get_paper_syst_shapes(bkg, df, model=model, pred_cb=InfernoPred()) . Running: r=-0.2 . &lt;progress value=&#39;1&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [1/1 00:03&lt;00:00] Running: r=0 . &lt;progress value=&#39;1&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [1/1 00:03&lt;00:00] Running: r=0.2 . &lt;progress value=&#39;1&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [1/1 00:03&lt;00:00] Running: l=2.5 . &lt;progress value=&#39;1&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [1/1 00:02&lt;00:00] Running: l=3 . &lt;progress value=&#39;1&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [1/1 00:03&lt;00:00] Running: l=3.5 . &lt;progress value=&#39;1&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [1/1 00:02&lt;00:00] The function also saves the new predicted bin to the DataFrame, allowing us to see how the background shifts compared to the nominal prediction. . plot_preds(df, pred_names=[&#39;pred&#39;, &#39;pred_-0.2_3&#39;, &#39;pred_0.2_3&#39;, &#39;pred_0_2.5&#39;, &#39;pred_0_3.5&#39;], bin_edges=np.linspace(0,10,11)) . Following our interpretation of the binning earlier, it seems possible that the aim of herding the majority of the background into a few bins was to increase the number of bins which are less affected by the systematic shifts. . Benchmark 2 . In benchmark 2 we allow the $r$ and $ lambda$ parameters to float. . %%capture --no-display from pytorch_inferno.inference import calc_profile profiler = partialler(calc_profile, f_s=f_s, n=1050, mu_scan=torch.linspace(20,80,61), true_mu=50, n_steps=100) nll = profiler(**b_shapes).cpu().detach() plot_likelihood(nll-nll.min()) . &lt;progress value=&#39;61&#39; class=&#39;&#39; max=&#39;61&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [61/61 00:09&lt;00:00] Wow! So previously introducing these shape nuisances caused the width to jump up to ~24.5 (a ~60% increase), whereas with INFERNO the width only increases by about 10%. . Benchmark 3 . Benchmark 3 is similar to BM2, except we introduce auxiliary measurements on the shapes. . %%capture --no-display alpha_aux = [torch.distributions.Normal(0,2), torch.distributions.Normal(0,2)] nll = profiler(alpha_aux=alpha_aux, **b_shapes).cpu().detach() plot_likelihood(nll-nll.min()) . &lt;progress value=&#39;61&#39; class=&#39;&#39; max=&#39;61&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [61/61 00:13&lt;00:00] The constraints restore some sensitivity, but not as much as we saw for the cross-entropy classifier. The width is still much smaller, though. . Benchmark 4 - Normalisation uncertainty . For the final benchmark, we also let the background yield be a nuisance parameter (with an auxiliary measurement) . %%capture --no-display nll = profiler(alpha_aux=alpha_aux, float_b=True, b_aux=torch.distributions.Normal(1000,100), **b_shapes).cpu().detach() plot_likelihood(nll-nll.min()) . &lt;progress value=&#39;61&#39; class=&#39;&#39; max=&#39;61&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [61/61 00:20&lt;00:00] The cross-entropy classifier really struggled with this, resulting in a width of 27.74 (~85% increase). INFERNO does much better, with an increase of only ~20%. That&#39;s pretty good! . Closing . By the end of the last post we weren&#39;t in a particularly good situation: we&#39;d taken on a simple toy problem and trained a nice powerful classifier using the typical approaches of HEP, however the moment that we introduced systematic uncertainties on parts of our statistical model, the precision of our measurement fell apart. We had trained our model on a surrogate classification task, but the actual problem we wanted to solve was one of statistical inference. . What we&#39;ve seen in this post is that by training the DNN to perform the actual task, rather than a surrogate task, and considering all the elements that go into the inference, we can arrive at a model which has learnt to be more resilient to the effects of nuisance parameters. On the most difficult benchmark, INFERNO provides a ~30% smaller width than the cross-entropy classifier. . I&#39;ve also presented here a way of implementing INFERNO that can be plugged into an existing DNN training scheme, without having to write an entirely new training loop. Granted, the training loop contains more callback flexibility than one would traditionally implement, but why not have this extra flexibility? In my other DNN framework, LUMIN, I recently upgraded the callback system to include these extra calls, and it&#39;s certainly simplified a lot of the code. I&#39;ll be including an implementation of INFERNO there, too, eventually. . One issue with current setup, however, is that we need to be able to modify the input features in a continuous manner; i.e. we need to either know exactly how each nuisance parameter affects the inputs, or we need to model their affects accurately. In part 5 I&#39;ll present a way in which we can avoid this and still apply INFERNO. . Another point is that the comparison between INFERNO and cross-entropy isn&#39;t strictly accurate. In INFERNO we effectively let the DNN decide how to bin the data, whereas for the cross-entropy we imposed a binning of 10 equally sizes bins and it could well be that using different sized bins might provide a better result. Possibly as a part 6 I&#39;ll revisit this and optimise the cross-entropy binning to get a better comparison. .",
            "url": "https://gilesstrong.github.io/website/statistics/hep/inferno/2020/12/31/inferno-4.html",
            "relUrl": "/statistics/hep/inferno/2020/12/31/inferno-4.html",
            "date": " • Dec 31, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Into the INFERNO: parameter estimation under uncertainty - Part 3",
            "content": ". Welcome back to the third part of this blog-post series. In case you missed it, the first and second parts may be found here and here. Last time we finished laying the necessary groundwork for parameter estimation, and in this post we&#39;ll look at the example presented in the INFERNO paper, and demonstrate how the typical approach in HEP would go about solving it. . For the rest of this blog-post series, we&#39;ll be using a PyTorch implementation of INFERNO which I&#39;ve put together, but I&#39;ll do my best to show the relevant code, and explain what each component does. The package can be installed using: . !pip install pytorch_inferno==0.0.1 . Docs are located here and the Github repo is here. I&#39;m installing v0.0.1 here, since it was what I used when writing this post, but for your own work, you should grab the latest version. . As an additional note, these blog posts are fully runnable as Jupyter notebooks. You can either download them, or click the Google Colab badge at the top to follow along! . The example . Similar to our examples in the previous two posts, the paper considers a toy example in which two classes of process (signal and background) contribute to an observed count. The count density is defined using three arbitrary features, $ bar{x}=(x_0,x_1,x_2)$, and differences in the probability distributions functions (PDFs) of the two classes can be exploited in order to improve the precision of the parameter of interest, $ mu$, which is the signal strength scaling the contributions of the signal process: $N_o= mu s + b$, where $N_o$ is the observed count and $s$ and $b$ are the expected contributions from the signal and background processes. . The PDFs of the two classes are specified using a 2D Gaussian distribution and an exponential distribution: $$f_b( bar{x}) = mathcal{N} left((x_0,x_1) ,| ,(2,0), begin{bmatrix}5&amp;0 0&amp;9 end{bmatrix} right) mathrm{Exp}(x_2 ,| ,3)$$ $$f_s( bar{x}) = mathcal{N} left((x_0,x_1) ,| ,(0,0), begin{bmatrix}1&amp;0 0&amp;1 end{bmatrix} right) mathrm{Exp}(x_2 ,| ,2)$$ . N.B. Eq.15 in the paper states the means of the signal Gaussian are $(1,1)$, however the code on which the results are derived used $(0,0)$. . In the package, this is implemented as: . class _PaperData(): r&#39;&#39;&#39;Callable class generating pseudodata from Inferno paper&#39;&#39;&#39; def __init__(self, mu:List[float], conv:List[List[float]], r:float, l:float): store_attr(but=[&#39;mu&#39;, &#39;r&#39;]) self.r = np.array([r,0]) self.mu = np.array(mu) def sample(self, n:int) -&gt; np.ndarray: return np.hstack((np.random.multivariate_normal(self.mu+self.r, self.conv, n), np.random.exponential(1/self.l, size=n)[:,None])) def __call__(self, n:int) -&gt; np.ndarray: return self.sample(n) . r is a nuisance parameter which can affect the mean of the Gaussian, but we&#39;ll get to that later on. . from pytorch_inferno.pseudodata import _PaperData _PaperData(mu=[0,0], conv=[[1,0],[0,1]], r=0, l=2).sample(2) . array([[-0.78121616, 1.2277006 , 0.12666304], [ 0.01064229, -2.22750248, 0.82898485]]) . Sampling two points, returns the values of the three features for both points. Let&#39;s generate samples for signal and background, and plot out the features: . %%capture --no-display from pytorch_inferno.pseudodata import PseudoData, paper_bkg, paper_sig import seaborn as sns n=1000 df = PseudoData(paper_sig, 1).get_df(n).append(PseudoData(paper_bkg, 0).get_df(n), ignore_index=True) sns.pairplot(df, hue=&#39;gen_target&#39;, vars=[f for f in df.columns if f != &#39;gen_target&#39;]) . &lt;seaborn.axisgrid.PairGrid at 0x1076efd68&gt; . From the distributions above we can see that the signal (1-orange) overlaps considerably with the background (0-blue), which no particular feature offering good separation power; the features as they stand are unsuitable for inferring $ mu$ via binning. $x_1$ perhaps could be used, since the background displays a greater spread than the signal, however we can hope to do better. . Classifier training . As stated last time, what we want is a single feature in which the classes of signal and background are most linearly separable. This has to be a function of the observed features $ bar{x}$: $$f( bar{x})=y,$$ where $y=0$ for background and $y=1$ for signal. I.e. $f$ maps $ bar{x}$ into a space in which the two processes are completely distinct. Unfortunately, such as function is not always achievable given the information available in $ bar{x}$. Instead we can attempt to approximate $f$ with a parameterisation: $$f_ theta( bar{x})= hat{y} approx y,$$ Since $f_ theta$ is only an approximation, the resulting predictions $ hat{y}$ will be a continuous variable in which (hopefully) background data will be clustered close to zero and signal clustered close to one. . Depending on any domain knowledge available, suitable (monotonically related) forms of $f_ theta$ may be available in the form of high-level features: E.g. when searching for a Higgs boson decaying to a pair of tau leptons, the invariant mass of the taus should form a peak around 125 GeV for signal whereas background should not form a peak. Such a high-level feature can then either be transformed into the range [0,1] or just binned for inference directly. . A better approach (usually), especially when domain theory cannot help, however is to learn the form of $f_ theta$ from example data using a sufficiently flexible model by optimising the parameters $ theta$ such that: $$ theta= arg min_ theta[L(y,f_ theta( bar{x})],$$ where the loss function $L$ quantifies the difference between the predictions of the model $f_ theta( bar{x})$ and the targets $y$. . Common forms of $f_ theta( bar{x})$ in HEP are Boosted Decision Trees and now more recently (deep) neural networks ((D)NNs. Eventually I&#39;ll write a new series on DNNs, but for now I can point you to my old series here. I&#39;ll proceed assuming a basic understanding of DNNs, I&#39;m afraid, so as not to go beyond scope. . Let&#39;s train a DNN as a binary classifier to predict whether data belongs to signal or background. The pytorch_inferno package includes a basic boilerplate code for yielding data and training DNNs. . As per the paper, we&#39;ll use a network with three hidden layers network and ReLU activations to map the three input features to a single output with a sigmoid activation. . from pytorch_inferno.utils import init_net from torch import nn net = nn.Sequential(nn.Linear(3,100), nn.ReLU(), nn.Linear(100,100),nn.ReLU(), nn.Linear(100,1), nn.Sigmoid()) init_net(net) # Initialise weights and biases . The ModelWrapper class then provides support for training the network, and saving, loading, and predicting. . from pytorch_inferno.model_wrapper import ModelWrapper model = ModelWrapper(net) . A training and testing dataset can be generated and wrapped in a class to yield mini-batches. . from pytorch_inferno.data import get_paper_data data, test = get_paper_data(n=200000, bs=32, n_test=1000000) . Now let&#39;s train the model. In contrast to the paper, we&#39;ll use ADAM for optimisation, at a slightly higher learning rate, to save time. We&#39;ll also avoid overtraining by saving when the model improves using callbacks. The Loss is binary cross-entropy, since we want to train a binary classifier. . from fastcore.all import partialler from torch import optim from pytorch_inferno.callback import LossTracker, SaveBest, EarlyStopping model.fit(200, data=data, opt=partialler(optim.Adam,lr=8e-4), loss=nn.BCELoss(), cbs=[LossTracker(),SaveBest(&#39;weights/best_bce.h5&#39;),EarlyStopping(5)]) . &lt;progress value=&#39;20&#39; class=&#39;&#39; max=&#39;200&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 10.00% [20/200 04:04&lt;36:44] &lt;progress value=&#39;3125&#39; class=&#39;&#39; max=&#39;3125&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [3125/3125 00:03&lt;00:00] 1: Train=0.3446095490694046 Valid=0.3378930442857742 2: Train=0.3353524197614193 Valid=0.3383234238386154 3: Train=0.3335498013615608 Valid=0.3354001357984543 4: Train=0.3327178750550747 Valid=0.334430864071846 5: Train=0.3327170829820633 Valid=0.3354277195715904 6: Train=0.3322705823850632 Valid=0.33362953107357024 7: Train=0.3319925680863857 Valid=0.3360459116411209 8: Train=0.3318649928891659 Valid=0.3335313368606567 9: Train=0.3317312321817875 Valid=0.33429972858905793 10: Train=0.33157649008274076 Valid=0.3346566602230072 11: Train=0.3315507458817959 Valid=0.33596284610271454 12: Train=0.331496762791872 Valid=0.33324935112953186 13: Train=0.3312726239180565 Valid=0.3352210135364532 14: Train=0.3312872124123573 Valid=0.3343600254631042 15: Train=0.3312903489100933 Valid=0.33351335374832153 16: Train=0.33124875135302545 Valid=0.33321514286994935 17: Train=0.3312225800585747 Valid=0.3346719899749756 18: Train=0.33116737874269486 Valid=0.3332399683332443 19: Train=0.33107781036496164 Valid=0.3342269439792633 20: Train=0.3311147364091873 Valid=0.33339463568210603 21: Train=0.33108707209706306 Valid=0.3348931646203995 Early stopping Loading best model with loss 0.33321514286994935 . Having trained the model, we can now check the predictions on the test set . preds = model._predict_dl(test) . &lt;progress value=&#39;15625&#39; class=&#39;&#39; max=&#39;15625&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [15625/15625 00:14&lt;00:00] import pandas as pd df = pd.DataFrame({&#39;pred&#39;:preds.squeeze()}) df[&#39;gen_target&#39;] = test.dataset.y df.head() . pred gen_target . 0 0.298936 | 1.0 | . 1 0.757607 | 1.0 | . 2 0.758828 | 1.0 | . 3 0.944337 | 1.0 | . 4 0.446215 | 1.0 | . from pytorch_inferno.plotting import plot_preds plot_preds(df) . From the above plot, we can see that the model was able to maps both classes towards their target values. It also shows good agreement with Fig. 3a in the paper. . Parameter estimation . No nuisance parameters . Now that we have learnt a feature (the DNN predictions) in which the signal and background a highly separated, we&#39;re now in a position to estimate our parameter of interest by binning the PDFs of signal and background, as per the last post. . Similar to the paper, we&#39;ll bin the predictions in 10 bins of equal size: . from pytorch_inferno.inference import bin_preds import numpy as np bin_preds(df, np.linspace(0,1,11)) # Bins each prediction df.head() . pred gen_target pred_bin . 0 0.298936 | 1.0 | 2 | . 1 0.757607 | 1.0 | 7 | . 2 0.758828 | 1.0 | 7 | . 3 0.944337 | 1.0 | 9 | . 4 0.446215 | 1.0 | 4 | . Now we want to get the PDFs for signal and background . from pytorch_inferno.inference import get_shape f_s,f_b = get_shape(df,targ=1),get_shape(df,targ=0) # Gets normalised shape for each class f_s,f_b . (tensor([0.0065, 0.0099, 0.0137, 0.0189, 0.0274, 0.0431, 0.0716, 0.1421, 0.3973, 0.2695]), tensor([0.6139, 0.0616, 0.0427, 0.0358, 0.0343, 0.0361, 0.0383, 0.0470, 0.0686, 0.0217])) . In this example (and generally in HEP) we use simulated data to estimate the PDFs of signal and background. The expected yields (overall normalisation) are separate parameters. This is to say that although we trained our model on 200,000 datapoints, and estimated the PDFs on 1,000,000 data points, these sample sizes need not have any relationship to the expected yields of signal and background; for the paper example, the background yield is 1000, and the signal yield is our parameter of interest, $ mu=50$. . At this stage of a proper HEP analysis, we would still be optimising our analysis and so will not have access to the observed data. Instead, as mentioned last time, we can check expected performance by computing the Asimov dataset; the expected yield per bin, where any parameters are at their expected yield. I.e., the Asmiov yield in bin i will be: $$N_a=50 cdot f_{s,i}+1000 cdot f_{b,i}$$ . asimov = (50*f_s)+(1000*f_b) asimov, asimov.sum() . (tensor([614.2289, 62.0748, 43.3652, 36.7367, 35.6574, 38.2549, 41.9260, 54.0901, 88.5101, 35.1559]), tensor(1050.)) . Now we can compute the negative loglikelihood for a range of $ mu$ values, as usual . import torch n = 1050 mu = torch.linspace(20,80,61) nll = -torch.distributions.Poisson((mu[:,None]*f_s)+(1000*f_b)).log_prob(asimov).sum(1) nll . tensor([31.9203, 31.7625, 31.6112, 31.4658, 31.3276, 31.1947, 31.0683, 30.9478, 30.8333, 30.7245, 30.6218, 30.5242, 30.4327, 30.3465, 30.2655, 30.1904, 30.1201, 30.0554, 29.9959, 29.9413, 29.8918, 29.8473, 29.8075, 29.7725, 29.7428, 29.7177, 29.6973, 29.6814, 29.6701, 29.6635, 29.6615, 29.6634, 29.6703, 29.6809, 29.6964, 29.7159, 29.7392, 29.7673, 29.7990, 29.8351, 29.8750, 29.9190, 29.9668, 30.0181, 30.0739, 30.1335, 30.1964, 30.2634, 30.3340, 30.4083, 30.4864, 30.5676, 30.6529, 30.7412, 30.8336, 30.9290, 31.0277, 31.1302, 31.2356, 31.3449, 31.4572]) . from pytorch_inferno.plotting import plot_likelihood plot_likelihood(nll-nll.min()) . Comparing the width to Tab.2 in the paper (NN classifier, Benchmark 0 = 14.99), our value seems to be in good agreement! . Nuisance parameters . There are two ways in which nuisance parameters can affect our measurement: 1) the nuisance parameter leads to a shift in the PDF of a class, but the overall normalisation stays the same; 2) the PDF stays the same, but the normalisation changes. Both of these types of nuisances can be constrained by auxiliary measurements, as described in part-1. . The paper considers a range of benchmarks for how the presence of different nuisances can affect the precision of the measurement: . The mean of the Gaussian distribution of the background is allowed to shift in one dimension by an amount $r$ | In addition to 1) the rate of the Exponential distribution, $ lambda$, of the background is allowed to change | Similar to 2), except $r$ and $ lambda$ are constrained by auxiliary measurements | Similar to 3), except now the overall background rate, $b$ is allowed to shift and is constrained by an auxiliary measurement | The uncertainty of $r$ and $ lambda$ leads to changes in the input features of our model, and so leads to changes in the PDF of the background distribution of the inference feature. When attempting to minimise the negative log-likelihood, by optimising the nuisance parameters, these shifts must be accounted for: if the $r$ shift is +0.2, then all of the $x_0$ features for the background are shifted by +0.2, which means that the predictions of the DNN will also change in some more complex way. Similarly, if $ lambda$ is increased by 0.5, then the whole of $x_3$ for background are multiplied by a factor $3.5/3$. . We are lucky in this example, that the analytic effects on the input features of these two nuisances can be computed; sometimes this is not the case. In HEP, simulated data are generated by stochastic modelling, and the exact effect on a given variable of say jet energy scale or tau ID cannot be analytically derived. The approach, then, is to generate additional datasets in which the values affected by the systematics are moved up and down according to their uncertainty (e.g. one would have a nominal dataset, and then JES_UP and JES_DOWN datasets in which the energy scale is shift up/down by 1-sigma). These additional datasets are then also fed through the DNN, in order to arrive at up/down shifts in the PDFs. Since the analytical effects on the input features in our example systematics are known, we can simply adjust the existing dataset and pass it through our model. . Shape variations . Let&#39;s now compute all the shifts in the background PDF. Following the paper, $r$ is shifted by $ pm0.2$, and $ lambda$ by $ pm0.5$. Remember that these shifts only affect the background process. get_paper_syst_shapes will shift the data, recompute the new model predictions, and then compute the new PDF for background . from pytorch_inferno.inference import get_paper_syst_shapes bkg = test.dataset.x[test.dataset.y.squeeze() == 0] # Select the background test data b_shapes = get_paper_syst_shapes(bkg, df, model=model, bins=np.linspace(0,1,11)) . Running: r=-0.2 . &lt;progress value=&#39;1&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [1/1 00:04&lt;00:00] Running: r=0 . &lt;progress value=&#39;1&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [1/1 00:04&lt;00:00] Running: r=0.2 . &lt;progress value=&#39;1&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [1/1 00:05&lt;00:00] Running: l=2.5 . &lt;progress value=&#39;1&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [1/1 00:04&lt;00:00] Running: l=3 . &lt;progress value=&#39;1&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [1/1 00:04&lt;00:00] Running: l=3.5 . &lt;progress value=&#39;1&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [1/1 00:04&lt;00:00] The function also saves the new predicted bin to the DataFrame, allowing us to see how the background shifts compared to the nominal prediction. . plot_preds(df, pred_names=[&#39;pred&#39;, &#39;pred_-0.2_3&#39;, &#39;pred_0.2_3&#39;, &#39;pred_0_2.5&#39;, &#39;pred_0_3.5&#39;]) . So we can see that the changes in the nuisance parameters really do result in shifts in the background PDF. In b_shapes we have a a dictionary of the nominal, up, and down shifts according to changes in $r$ and $ lambda$. . b_shapes . OrderedDict([(&#39;f_b_nom&#39;, tensor([0.6139, 0.0616, 0.0427, 0.0358, 0.0343, 0.0361, 0.0383, 0.0470, 0.0686, 0.0217])), (&#39;f_b_up&#39;, tensor([[0.6361, 0.0595, 0.0414, 0.0344, 0.0322, 0.0345, 0.0358, 0.0434, 0.0628, 0.0198], [0.6093, 0.0615, 0.0430, 0.0356, 0.0340, 0.0359, 0.0382, 0.0470, 0.0687, 0.0268]])), (&#39;f_b_dw&#39;, tensor([[0.5921, 0.0634, 0.0446, 0.0374, 0.0356, 0.0377, 0.0409, 0.0502, 0.0744, 0.0237], [0.6184, 0.0617, 0.0428, 0.0357, 0.0344, 0.0364, 0.0387, 0.0468, 0.0682, 0.0168]]))]) . These shifts, however, are discrete and our optimisation of the values of $r$ and $ lambda$ needs to happen in a continuous manner, but generating new simulated data at the desired nuisance values is too time consuming. Instead we can interpolate between the systematic shifts and the nominal values in each bin in order to estimate the expected shape of the PDF for proposed values of the nuisances. Additionally, such shifts need to happen in a differentiable manner, so that we can optimise the nuisance values using the Newtonian method, as per part-1. . The interp_shape function performs a quadratic interpolation between up/down shifts and the nominal value, and a linear extrapolation outside the range of up/down shifts, and does so in a way that allows the resulting values to be differentiated w.r.t. the nuisance parameters. For example in bin zero, we can plot (in red) the computed PDF values at up (1.0), nominal (0.0), and down (-1.0), and also then plot the results of the interpolation and extrapolation for shifts in $r$ (blue scatter). The corresponding PDF value is shown in the y-axis. . from pytorch_inferno.inference import interp_shape from torch import Tensor import matplotlib.pyplot as plt i = 0 d = b_shapes[&#39;f_b_dw&#39;][0][i] n = b_shapes[&#39;f_b_nom&#39;][i] u = b_shapes[&#39;f_b_up&#39;][0][i] interp = [] rs = np.linspace(-2,2) for r in rs: interp.append(interp_shape(Tensor((r,0))[None,:], **b_shapes)[0][i].data.item()) plt.scatter(rs, interp) plt.plot([-1,0,1],[d,n,u], label=i, color=&#39;r&#39;) . [&lt;matplotlib.lines.Line2D at 0x1a37954438&gt;] . Benchmark 2 . Let&#39;s now evaluate our trained model under Benchmark 2, in which both $r$ and $ lambda$ are completely free. The process of minimising the NLL at each value of $ mu$ is now slightly more complicated than before since we now have multiple nuisances, and so need to compute the full Jacobian and Hessian matrices (as warned in part-1). . Unfortunately here either my PyTorch skills fail me, or PyTorch lacks sufficient flexibility, but I haven&#39;t yet managed to perform these calculations in a batch-wise manner. Meaning that the NLL optimsation must be done in series for every value of $ mu$, rather than in parallel as before. I think the problem is that PyTorch is overly secure and refuses to compute gradients when not all variables are present, even if they aren&#39;t necessary to compute the gradients. So if anyone has any ideas, please let me know! . Anyway, let&#39;s use the calc_profile function to minimise the nuisances for a range of signal strengths and return the NLL . %%capture --no-display from pytorch_inferno.inference import calc_profile profiler = partialler(calc_profile, f_s=f_s, n=1050, mu_scan=torch.linspace(20,80,61), true_mu=50, n_steps=100) nll = profiler(**b_shapes).cpu().detach() plot_likelihood(nll-nll.min()) . &lt;progress value=&#39;61&#39; class=&#39;&#39; max=&#39;61&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [61/61 00:10&lt;00:00] Right, so the uncertainty has gone way up! What a shame. . Benchmark 3 . We can slightly improve things by including auxiliary measurements on $r$ and $ lambda$. Benchmark 3 in the paper considers constraints as Gaussian distributions with standard deviations equal to 2, so let&#39;s include those in the NLL minimisation. . N.B. in contrast to the paper and paper code, in the implementation here, systematics are treated as new parameters perturbing existing parameters from their nominal values, rather than being the existing parameters themselves. . %%capture --no-display alpha_aux = [torch.distributions.Normal(0,2), torch.distributions.Normal(0,2)] nll = profiler(alpha_aux=alpha_aux, **b_shapes).cpu().detach() plot_likelihood(nll-nll.min()) . &lt;progress value=&#39;61&#39; class=&#39;&#39; max=&#39;61&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [61/61 00:13&lt;00:00] The constraints restore some sensitivity, but we&#39;re still doing much worse than before, when we didn&#39;t consider the nuisance parameters. . Normalisation uncertainty . The second way that nuisances can affect the measurement, is by globally rescaling the background contributions (i.e. letting $b$ float when minimising the NLL). As we saw in part-1 these kinds of nuisances really need to be constrained, however with the binned approach, we do have some slight resilience to them, unlike before where they completely killed our sensitivity. . %%capture --no-display nll = profiler(alpha_aux=alpha_aux, float_b=True, b_aux=torch.distributions.Normal(1000,100), **b_shapes).cpu().detach() plot_likelihood(nll-nll.min()) . &lt;progress value=&#39;61&#39; class=&#39;&#39; max=&#39;61&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [61/61 00:19&lt;00:00] Ouch, that really doesn&#39;t look good! . Closing . So what we&#39;ve seen here is that by using a neural network we can map a set of weakly discriminating features down into a single powerful feature, and use it for statistical inference of parameters in our model. When applicable, this is the typical approach now in contemporary HEP, (and has been used to get some really great results over the years!). . Unfortunately, what we&#39;ve also seen is that the presence of nuisance parameters (systematic uncertainties) can really spoil the performance of the measurement. Both by altering the input features, leading to flexibility in the shape of the inference variable, and by adjusting the overall normalisation of different processes contributing to the observed data (or its Asimov expectation). . The way in which we map the weak features down to the single, strong feature, and the way that we bin the resulting distribution, should ideally be done in a manner which accounts for the effects of the nuisance parameters that will later be included in the parameter estimation. This is precisely what the INFERNO method sets out to accomplish, and in the next post, we shall begin looking at how this can be achieved. .",
            "url": "https://gilesstrong.github.io/website/statistics/hep/inferno/2020/12/18/inferno-3.html",
            "relUrl": "/statistics/hep/inferno/2020/12/18/inferno-3.html",
            "date": " • Dec 18, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Into the INFERNO: parameter estimation under uncertainty - Part 2",
            "content": ". Welcome back to the second part of this blog-post series. In case you missed it, the first part may be found here. Last time we introduced the basic statistical framework for estimating parameters, and in this part we will expand on it to see how we can improve the precision of our measurements by considering multiple regions of the data-space. . Whilst I had said that we would begin looking at the paper this time, given some nice feedback I got from the previous post, I thought I&#39;d spend a bit more time detailing a few more concepts, on the off chance that these posts are of more general use to people. . Updates: . 2020/12/15: Corrected report of relative uncertainty when comparing combined measurement to the just the cut measurement; added pip install for missing package for Collab | 2020/12/14: Corrected plotting function to compute nll widths at 0.5 (previously was 1) | . Defining the data-space . First let&#39;s look at an interesting effect. Last time we kept the background, b, and observed count, n constant, but what happens if we adjust the data space that we observe? Let&#39;s keep the optimal signal strength at 100 (i.e. $n=100+b$), and adjust the level of background. . def compute_nll(mu:Tensor, b:float, n:float) -&gt; Tensor: t_exp = mu+b return -torch.distributions.Poisson(t_exp).log_prob(n) . mu_range = torch.linspace(0,200, 50) nlls,labels = [],[] for b in [500,1000,1500]: nlls.append(compute_nll(mu_range, b=b, n=b+100)) labels.append(f&#39;b={b}, &#39;+r&#39;$ frac{s}{ sqrt{b}}=$&#39;+f&#39;{100/np.sqrt(b):.2f},&#39;) . _ = plot_nlls(nlls, mu_range, labels) . So, improving the ratio of signal to background directly leads to an improvement in the precision of our estimate for $ mu$, the signal strength. The challenge, then is to find a region of the data-space where a lot of signal is present, and as little background contamination as possible. The data-space can parameterised in terms of features describing the data. To visualise this, let&#39;s imagine the case of a feature in which the signal is mainly concentrated about a particular value, and the background is uniform: . import seaborn as sns nb = 1000 ns = 100 s = np.random.normal(5, 1.5, ns) b = np.random.uniform(0, 10, nb) def plot_data(s:np.ndarray, b:np.ndarray) -&gt; None: sns.distplot(b,label=&#39;Background&#39;,hist=True,kde=False,rug=True,hist_kws={&#39;density&#39;:True}) sns.distplot(s,label=&#39;Signal&#39;,hist=True,kde=False,rug=True,hist_kws={&#39;density&#39;:True}) plt.xlabel(&#39;x&#39;) plt.legend() plt.show() plot_data(s,b) . /Users/giles/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result. return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval . Now, if we simply take the data-space as is, then we end up counting all the signal and all the background. If, however, we take advantage of the differences in shape (density) of the signal and background, then we can work to optimise the ratio of signal to background, and so improve the precision of our measurement. . As an example, if we were to only consider data falling in the region $2 leq x leq8$, then we would have a much better signal-background ratio, than if we were to consider the whole data-space . cut_s = s[(2&lt;=s)*(s&lt;=8)] cut_b = b[(2&lt;=b)*(b&lt;=8)] print(f&#39;Ratio without cut {len(s)/len(b)}, with cut {len(cut_s)/len(cut_b):.2f}&#39;) nlls,labels = [],[] nlls.append(compute_nll(mu_range, b=nb, n=nb+ns)) labels.append(f&#39;No cut&#39;) nlls.append(compute_nll(mu_range, b=len(cut_b), n=len(cut_b)+len(cut_s))) labels.append(f&#39;With cut&#39;) widths = plot_nlls(nlls, mu_range, labels) print(f&#39;Fractional uncertainty without cut = {widths[0]/ns*100:.1f}%, with cut = {widths[1]/len(cut_s)*100:.1f}%&#39;) . Ratio without cut 0.1, with cut 0.15 . Fractional uncertainty without cut = 33.2%, with cut = 28.5% . So by carefully cutting away some of the data, we can improve the measurement. There was a slight shift in the optimal signal rate because we also cut away some expected signal. This data, however, is usually the result of an expensive and time-consuming process, and we don&#39;t want to just throw it away without making the most of it! . A more efficient approach is to combine two measurements: one on the data inside the cut, and the other on data outside it. To do this we need to refine the nll calculation slightly, in order to correctly split $ mu$ between these to regions. . outsidecut_s = s[(2&gt;s)+(s&gt;8)] outsidecut_b = b[(2&gt;b)+(b&gt;8)] # Compute the fraction of signal and background falling in each data region s_shape = Tensor((len(cut_s), len(outsidecut_s))) s_shape /= s_shape.sum() b_shape = Tensor((len(cut_b), len(outsidecut_b))) b_shape /= b_shape.sum() n = Tensor((len(cut_b)+len(cut_s), len(outsidecut_b)+len(outsidecut_s))) s_shape.sum(), b_shape.sum(), n.sum() . (tensor(1.), tensor(1.), tensor(1100.)) . def compute_nll(mu:Tensor, s_shape:Tensor, b_shape:Tensor, n:Tensor, b:float) -&gt; Tensor: t_exp = (mu[:,None]*s_shape)+(b*b_shape) # The expected count in each data region nll = -torch.distributions.Poisson(t_exp).log_prob(n) return nll.sum(1) . Note the nll.sum above. As per Eq.6 in Asymptotic formulae for likelihood-based tests of new physics, the likelihood should be the product of the Poisson probabilities, however we are working with the log probabilities, so instead we just sum them ($ log(10*20)= log(10)+ log(20)$). . nlls.append(compute_nll(mu_range, s_shape=s_shape, b_shape=b_shape, n=n, b=nb)) labels.append(f&#39;Combined measurement&#39;) widths = plot_nlls(nlls, mu_range, labels) print(f&#39;Fractional uncertainty with cut = {widths[1]/len(cut_s)*100:.1f}%, combined {widths[2]/ns*100:.1f}%&#39;) . Fractional uncertainty with cut = 28.5%, combined 28.4% . The combination of measurements further improves the precision of our measurement beyond what we get simply from the measurement inside the cut region (admittedly by only a slight amount in this case, though). . This approach can be referred to as binning the data-space in terms of some variable. And we are not limited to just two bins, we can have as many as want: . import math def bin_data(s:np.ndarray, b:np.ndarray, n_bins:int) -&gt; Tuple[Tensor,Tensor,Tensor]: edges = np.linspace(0,10,n_bins+1) edges[0] = -math.inf # Ensure all the data end up in the bins edges[-1] = math.inf s_shape,b_shape = torch.zeros((n_bins)),torch.zeros((n_bins)) for i in range(n_bins): s_shape[i] = len(s[(edges[i]&lt;=s)*(s&lt;=edges[i+1])]) b_shape[i] = len(b[(edges[i]&lt;=b)*(b&lt;=edges[i+1])]) n = s_shape+b_shape s_shape += 1e-7 # Ensure that every bin has some fill to avoid NaNs and Infs b_shape += 1e-7 s_shape /= s_shape.sum() # Normalise to get the fractional fill (shape) b_shape /= b_shape.sum() return s_shape, b_shape, n . n_bins = 5 s_shape, b_shape, n = bin_data(s,b,n_bins=n_bins) . nll = compute_nll(mu_range, s_shape=s_shape, b_shape=b_shape, n=n, b=nb) . nlls.append(compute_nll(mu_range, s_shape=s_shape, b_shape=b_shape, n=n, b=nb)) labels.append(f&#39;{n_bins} bins,&#39;) _ = plot_nlls(nlls, mu_range, labels) . Choice of feature . So we have seen that binning the data in terms of a feature can improve our measurement, but what about if we have several features to choose from? Let&#39;s look at a second example feature with slightly poorer separation: . def get_example(b_mean:float) -&gt; Tensor: s = np.random.normal(5, 1.5, ns) b = np.random.normal(b_mean, 1.5, nb) plot_data(s,b) s_shape, b_shape, n = bin_data(s,b,n_bins=5) return compute_nll(mu_range, s_shape=s_shape, b_shape=b_shape, n=n, b=nb) . _ = plot_nlls([get_example(2),get_example(5)], mu_range, [&#39;Good shape&#39;,&#39;Bad shape&#39;]) . /Users/giles/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result. return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval . In the first feature, the signal and background densities are well separated, however in the second, they overlap. Hopefully it&#39;s clear from this example this the choice of feature has a large effect on the precision of the measurement. . In HEP, and may other areas, there are many different features which can be computed to describe the data. Ideally we want to have a single feature in which the signal and background are most linearly separable, such that when it is binned, many of the bins display a large signal to background ratio. Next time we will look into how we can go about searching for such a feature. . Closing . So, unfortunately we still didn&#39;t get to dealing with the INFERNO paper, but now that we&#39;ve finished laying the groundwork and have introduced most of the required concepts, we should be able to move onto the paper in the next post. . Asimov dataset . One subtlety that you may have noticed, is last time we were evaluating the log probability we used the number of observed data. In this post I have assumed that the observed count (per bin) is equal to the expected signal and background contributions (in each bin). . In HEP, we often perform analyses blind, meaning that we only work with simulated data until the entire analysis is fixed in place, and all optimisation has been performed. Only then do we look at the real data. I.e. we only know the observed counts right at the very end of the analysis. As we have seen, though, various choices, such as the feature used, and the binning procedure can have huge influences on the performance of the analysis. Therefore it is useful to have some idea of the observed count to allow us to compute the expected precision and so optimise the analysis. . As discussed in Asymptotic formulae for likelihood-based tests of new physics, a suitable substitute for the observed count is the sum of the expected contributions from signal and background in which the nuisance parameters and parameter of interest are at their nominal values. This is referred to in the paper as the Asimov dataset. Remembering that the signal and background are Poisson processes, and so could be used to generate an infinite number of different expected datasets through sampling, by taking the expected yields, the Asimov dataset is the &quot;single most representative dataset&quot;. The naming is inspired by the book Franchise by Isaac Asimov, in which the single most representative person is used to vote on behalf of an entire populace. .",
            "url": "https://gilesstrong.github.io/website/statistics/hep/inferno/2020/12/11/inferno-2.html",
            "relUrl": "/statistics/hep/inferno/2020/12/11/inferno-2.html",
            "date": " • Dec 11, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Into the INFERNO: parameter estimation under uncertainty - Part I",
            "content": ". In this series of posts, we&#39;ll be looking at one of the most common problems in high-energy physics; that of estimating some parameter of interest. In this first post we&#39;ll look at the statistical framework for making measurements, and seeing how these can be spoilt once we introduce uncertainties on other parameters in our measurement. . This situation can be improved, however, though an approach called Inference Aware Neural Optimisation (INFERNO) developed by Pablo de Castro Manzano and Tommaso Dorigo, and over the next few posts I&#39;ll be breaking down the paper and explaining the approach in runnable code. Accompanying this series will be a PyTorch implementation of their method, but for this initial post we don&#39;t need it. . Whilst this method is potentially applicable to a wide range of problems, I&#39;m a particle physicist, and so will be describing the problem primarily from that point of view. . Updates: . 2020/12/14: Corrected plotting function to compute nll widths at 0.5 (previously was 1) | . Parameter estimation . Posson contributions . Commonly in high-energy particle physics, such as the work performed at the Large Hadron Collider at CERN, we want to estimate some parameter of our theoretical model. This could perhaps be rate at which some new process contributes to the data we collect. If a given region of our observed data contains $N_o$ data points (events) and processes already included in our model (referred to as background) contribute $N_b$ events, then one might say that any surplus event count is due to some new process which is not included in our model (signal): . $$N_s=N_o-N_b.$$ . The problem is that different process contribute as a random Poisson process; we can compute the mean rate via Monte Carlo simulation but we never know the exact contributions of each process in the observed data, i.e. we instead are comparing the observed count with the expected process contributions, $ left&lt;N_s right&gt;+ left&lt;N_b right&gt;$. For ease of writing, I&#39;ll refer to these expected counts as $s$ and $b$. . For the problem of evaluating the presence of signal in our data, it is useful to parameterise the expected count as: . $$n= mu s+b,$$ . where $ mu$ is a coefficient controlling the level of signal contribution. $ mu=0$ would therefore mean no signal contribution. This signal strength is our parameter of interest; we want to construct an experiment that allows us to measure this parameter as precisely as possible. . Nuisance parameters . Unfortunately, the problem can be further complicated by the fact that our theoretical model may not be fully specified: we may only know the mean value of the mean contribution of background, i.e. the model for the background contains unknown parameters and we&#39;ve simply assumed values for them based on expectation or prior measurements. . These nuisance parameters ($ theta$) can affect the expected background contribution, making the signal contribution more difficult to estimate: . $$n( mu, theta)= mu s+b( theta).$$ . Estimating $ mu$ . Our ultimate aim is to adjust our model so that it describes the observed data as well as possible. This means adjusting the parameters of the model in order to improve its goodness-of-fit through a maximum likelihood fit. Our likelihood function for these uncertain Possion contributions to an observed count $N_o$ is: . $$L( mu, theta)= frac{( mu s+b( theta))^{N_o}}{N_o!}e^{ mu s+b( theta)}$$ . The parameters $ mu$ and $ theta$ can then be adjusted in in order to maximise the likelihood, meaning that our best estimate of the parameter of interest is: . $$ hat{ mu} = arg max_{ mu, theta}L( mu, theta)$$ . Uncertainty on $ mu$ . The above formulation will give us an estimation of the parameter of interest, $ mu$. However every experiment will give us a measurement of $ mu$. What really matters when constructing the experiment is the associated uncertainty on the resulting measurement of $ mu$. . We can compute the uncertainty on our measurement by testing a range of possible values for $ mu$ and maximising the likelihood at that value by only adjusting the nuisance parameters. . $$ hat{L}( mu_i, hat{ theta}) = max_{ theta}L( mu_i, theta)= max_{ theta} left[ frac{( mu_i s+b( theta))^{N_o}}{N_o!}e^{ mu_i s+b( theta)} right]$$ . The nuisance parameters compensate for the lack of freedom on $ mu$, allowing the likelihood to be maximised above what would be possible if the nuisance parameters were not present. This effect is illustrated in the figure below: . . Whilst the value of $ mu$ which maximises the likelihood remains the same, the presence of nuisances increases the width of the likelihood thereby reducing our experiment&#39;s sensitivity to the exact value of $ mu$. . Testing . Let&#39;s take a quick look at what is really going on here in terms of code. We&#39;ll be working in PyTorch since we&#39;ll be making use of it&#39;s automatic differentiation later on. First we&#39;ll import the necessary stuff. . %matplotlib inline import matplotlib.pyplot as plt from fastcore.all import is_listy from typing import List, Optional, Tuple from scipy.interpolate import InterpolatedUnivariateSpline import numpy as np import torch from torch import Tensor, autograd, distributions . Fully-specified background model . As a semi-abstract example let&#39;s say that we observe 1100 events ($n$), and our background model predicts that we should observe an average of 1000 events ($b$). In addition to the background, there is a possible signal process (which may or may not exist), which can contribute an extra $ mu$ number of events. . b = 1000 n = 1100 . In this case the best-fit value of $ mu$ is 100, however what also matters is the uncertainty on $ mu$ that our measurement carries. To evaluate this, we first compute the likelihood as a function of $ mu$. . $$L( mu)= frac{( mu+b)^{n}}{n!}e^{ mu+b}$$ . For computational reasons: . We usually compute the natural log of the likelihood (e.g. PyTorch distributions only offer a log_prob method) | We rearrange the problem into a minimisation problem, rather than a maximisation one (functional optimisation traditionally deals with the minimisation of some cost function) | All this is to say that rather than maximising the likelihood, we will instead be minimising the negative log-likelihood (nll). So to compute this, first compute the mean of the Poisson distribution ($ mu+b$=t_exp), and then we evaluate the natural log of the probability at the number of observed events $n$. . def compute_nll(mu:Tensor) -&gt; Tensor: t_exp = mu+b return -torch.distributions.Poisson(t_exp).log_prob(n) . The above function is vectorised, so we can compute the nll of a range of $ mu$ values simultaneously. . mu_range = torch.linspace(0,200) nlls = compute_nll(mu_range) . Let&#39;s plot them out. Notice how the nll reaches a minimum value at 100, and then increases as $ mu$ moves away from 100, indicating that the level of agreement of our model with the observed data is decreasing. . plt.plot(mu_range,nlls) plt.xlabel(&#39;$ mu$&#39;) plt.ylabel(&#39;$- ln L$&#39;) plt.show() . As mentioned earlier, we are most interested in comparing measurements in terms of the width of the likelihood function (uncertainty on $ mu$ measurement). For convenience, let&#39;s write a plotting function that shifts the likelihoods to zero (nlls-nlls.min()=$ Delta nll$), and computes the width at $ Delta nll=0.5$, which corresponds to one standard deviation. . def plot_nlls(nlls:List[Tensor], mu_scan:Tensor, labels:Optional[List[str]]=None) -&gt; List[float]: if not is_listy(nlls): nlls = [nlls] if labels is None: labels = [&#39;&#39; for _ in nlls] elif not is_listy(labels): labels = [labels] widths = [] plt.figure(figsize=(16,9)) plt.plot(mu_scan,[0.5 for _ in mu_scan], linestyle=&#39;--&#39;, color=&#39;black&#39;) for nll,lbl in zip(nlls,labels): dnll = nll-nll.min() # Shift nll to zero roots = InterpolatedUnivariateSpline(mu_scan, dnll-0.5).roots() # Interpolate to find crossing points at dll=0.5 if len(roots) &lt; 2: widths.append(np.nan) # Sometimes the dnll doesn&#39;t cross 1 else: widths.append((roots[1]-roots[0])/2) # Compute half the full width (uncertainty on mu) plt.plot(mu_range, dnll, label=f&#39;{lbl} Width={widths[-1]:.2f}&#39;) plt.legend(fontsize=16) plt.xlabel(&#39;$ mu$&#39;, fontsize=24) plt.ylabel(&#39;$ Delta (- ln L)$&#39;, fontsize=24) plt.show() return widths . _ = plot_nlls(nlls, mu_range) . Uncertain background model . Let&#39;s now imagine an example in which a nuisance parameter ($ theta$) affects the level of contribution the background process has. I.e. there is some part of the background model which is not fully specified. In this case we must perform a profile fit of this nuisance parameter, when evaluating the likelihood at each value of the parameter of interest, $ mu$. . As before, we will be aiming to minimise the negative log-likelihood: . $$ hat{L}( mu_i, hat{ theta}) = min_{ theta} left[- ln left[ frac{( mu_i+(1+ theta)b)^{n}}{n!}e^{ mu_i+(1+ theta)b} right] right]$$ . def compute_nll(mu:Tensor, theta:Tensor) -&gt; Tensor: t_exp = mu+((1+theta)*b) return -torch.distributions.Poisson(t_exp).log_prob(n) . Now that our computation of the nll includes the background rate uncertainty, we need someway to minimise it by varying the value of the nuisance parameter. Both the authors&#39; Tensorflow 1 implementation and Lukas Layer&#39;s Tensorflow 2 version use Newtonian optimisation, so let&#39;s go with that. . In this iterative method, $ arg min_ theta f( theta)$ can be found via updates to $ theta$ of the form: . $$ theta_{k+1} = theta_k- gamma frac{f&#39;}{f&#39;&#39;},$$ . where $f&#39;$ is the derivative of $f$ with respect to $ theta$ at the value $ theta_k$, and $f&#39;&#39;$ is the derivative of $f&#39;$ with respect to $ theta_k$ (the second derivative). $ gamma$ controls the rate of convergence (similar to a learning rate). . This is where PyTorch&#39;s automatic differentiation really becomes useful; we can compute nll, and then differentiate it w.r.t. the nuisance parameter in order to compute the update step for the value of nuisance parameter. . One subtlety in the above form is that when multiple nuisances are involved, they must be updated simultaneously, i.e. $f&#39;$ must be the derivative w.r.t. a vector of nuisance parameters $ bar{ theta}$ (the Jacobian). $f&#39;&#39;$ is then a matrix of derivatives of $f&#39;$ w.r.t $ bar{ theta}$ (the Hessian). . Further complication arises when computing $ frac{f&#39;}{f&#39;&#39;}$, since the inverse of $f&#39;&#39;$ must instead be computed, and multplied to $f&#39;$. Inverting matrices can be computationally difficult, however for a small number of parameters, it should be manageable (this is one reason why such second-order methods aren&#39;t used for optimising neural networks). . def optimise_theta(mu:float, n_steps:int=100, lr=0.1) -&gt; Tensor: def calc_derivatives(nll:Tensor, theta:Tensor) -&gt; Tuple[Tensor, Tensor]: first = autograd.grad(nll, theta, torch.ones_like(nll), create_graph=True)[0] second = autograd.grad(first, theta, torch.ones_like(first))[0] return first, second theta = torch.zeros_like(mu, requires_grad=True) # Initaialise the nuisance to zero for i in range(n_steps): # Newton optimise nuisances nll = compute_nll(mu, theta) first, second = calc_derivatives(nll, theta) # Differentiate NLL w.r.t. the nuisance parameter step = lr*first/second theta = theta-step return theta.cpu().detach() # Return the optimal value of nuisance parameter . Similar to the nll calculation, the function above is vectorised, so we can optimise multiple values of the nuisance parameter simultaneously (and independently). . theta = optimise_theta(mu_range) nlls_uncertain = compute_nll(mu_range, theta) . Let&#39;s plot out this likelihood. . _ = plot_nlls([nlls, nlls_uncertain], mu_range, labels=[&#39;No nuisance&#39;, &#39;Uncertain Bkg.&#39;]) . Oh no! We&#39;ve completely lost all sensitivity to the parameter of interest. This is because the nuisance is free to take any value, and so can compensate for the variation in $ mu$ in order to consistently produce an expected count of 1100 events to match the observed data. . Constrained nuisance parameters . In order to avoid situations like the above, other measurements of the background model can be included to help dictate reasonable values of the nuisance parameter (referred to as auxiliary measurements and constraints). Such auxiliary measurements typically come with their own associated uncertainties. . Continuing our example, let&#39;s say we already measured the nuisance parameter $ theta$ and arrived at a value of $0 pm0.05$, i.e. the nuisance doesn&#39;t appear to affect the background yield, however we&#39;re not completely certain. We can include the measurement to constrain the nuisance by assuming it follows a Gaussian distribution, centred at zero with a standard deviation of 0.05. . Whilst $ theta$ is still free to take any value, the likelihood receives a contribution from the auxiliary measurement in the form of a log-Normal evaluated at the proposed value of $ theta$. I.e. if the value of $ theta$ strays far from zero, then the likelihood receives a penalty. . def compute_nll(mu:Tensor, theta:Tensor) -&gt; Tensor: t_exp = mu+((1+theta)*b) nll = -torch.distributions.Poisson(t_exp).log_prob(n) nll -= distributions.Normal(0,0.05).log_prob(theta) # Subtract the log-probability according to aux measurement return nll . theta = optimise_theta(mu_range) nlls_constrained = compute_nll(mu_range, theta) . _ = plot_nlls([nlls, nlls_uncertain, nlls_constrained], mu_range, labels=[&#39;No nuisance&#39;, &#39;Uncertain Bkg.&#39;, &#39;Constrained Bkg.&#39;]) . So, with our auxiliary measurement of the nuisance parameter, we once again have sensitivity to our parameter of interest. However, the extra flexibility the model has from nuisance parameter means that the measurement is still worse than if the background model were fully specified (larger width). . Closing . Here we&#39;ve begun looking to the statistical framework for estimating a parameter of interest when nuisance parameters are present. However we&#39;ve so far ignored the methods by which the counts are gained. In the next post we&#39;ll look into how measurements can be improved by considering multiple regions of the data-space, and how the typical approaches (in HEP) aren&#39;t always optimal once nuisances are included. . Further reading . Of course, I&#39;d recommend reading through the INFERNO paper. I&#39;ll be breaking down the method itself, but the paper gives an in depth discussion about the context, problem, and other work. . For an introduction to the statistical framework used in HEP, I&#39;d recommend (at least Section 2) of Asymptotic formulae for likelihood-based tests of new physics by Glen Cowan, Kyle Cranmer, Eilam Gross, and Ofer Vitells. .",
            "url": "https://gilesstrong.github.io/website/statistics/hep/inferno/2020/12/04/inferno-1.html",
            "relUrl": "/statistics/hep/inferno/2020/12/04/inferno-1.html",
            "date": " • Dec 4, 2020"
        }
        
    
  
    
  

  
  
      ,"page0": {
          "title": "About Me",
          "content": ". Hello and welcome to my blog / personal website! My name is Giles, I’m a researcher of particle physics and machine learning, and obtained my PhD in physics from IST, Lisbon in 2021. . Currently I am working as a researcher at INFN-Padova. My day to day work takes place mostly within the context of the CMS experiment at CERN, and my particular research interests revolve around developing, adapting, and applying machine learning methods to the various domain-specific problems we face in analysing high-energy physics (HEP) data. . My current research now includes: . Searching for di-Higgs production in the bb𝜏𝜏 decay channel with CMS (paper); | Developing regression models for muon energy measurement in calorimeters (paper); | Systematics-aware training methods for models (see posts on INFERNO, and pytorch_inferno package); | End-to-end optimisation of detector design as part of the newly-formed MODE collaboration. In particular, I focus on detector optimisation for muon-tomography as a core developer of the TomOpt package; checkout section 4.3 in our recent whitepaper, or my blog post) | . Additionally I am the core developer for LUMIN, a PyTorch wrapper library allowing researchers to easily apply the latest training methods and architecture for deep neural networks to their analysis of HEP data. Please see this paper for an example. . Please see my publications and presentations pages for further details. I can be found on GitHub and Twitter, and am contactable by email at giles.chatham.strong (@) cern.ch. .",
          "url": "https://gilesstrong.github.io/website/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page1": {
          "title": "Publications",
          "content": "Database links . Orcid: 0000-0002-4640-6108 | INSPIRE: Giles Strong - Please note that this includes all publications that list me as an author; CMS Collaboration papers are signed by all members of the collaboration. | . Selected papers . Author of TomOpt section of: T. Dorigo et al. (37 authors including G. Strong) “Toward the End-to-End Optimization of Particle Physics Instruments with Differentiable Programming: a White Paper”, March (2022), arXiv:2203.13818 [physics.ins-det] | Joint author, and researcher for DNN signal classification of: CMS Collaboration “Search for nonresonant Higgs boson pair production in final state with two bottom quarks and two τ leptons in proton-proton collisions at √s=13 TeV”, March (2022), CMS-PAS-HIG-20-010 | Contributing researcher and minor author of: T. Dorigo, S. Guglielmini, J. Kieseler, L. Layer, G. Strong, “Deep Regression of Muon Energy with a K-Nearest Neighbor Algorithm”, March (2022), arXiv:2203.02841 [hep-ex] | Second author &amp; main researcher of: J Kieseler, G. Strong, F. Chiandotto, T. Dorigo, &amp; L. Layer, “Calorimetric Measurement of Multi-TeV Muons via Deep Regression”, January (2022), The European Physical Journal C 82:79 | Joint author, and researcher for: h→𝜏𝜏 and hh→bb𝜏𝜏@HL-LHC sections of: A. Stakia, T. Dorigo, et al. (23 additional authors inc. G. Strong), “Advanced Multi-Variate Analysis Methods for New Physics Searches at the Large Hadron Collider”, Nov. (2021), Reviews in Physics 7:100063. | Minor author of: T. Dorigo et al. (5 additional authors inc. G. Strong), “RanBox: Anomaly Detection in the Copula Space”, June (2021), arXiv:2106.05747 [physics.data-an] - preprint submitted to Computer Physics Communications. | Member author of: MODE Collaboration, “Toward Machine Learning Optimization of Experimental Design”, Mar. (2021), Nuclear Physics News, 31:1, 25-28 | Sole author of: G. Strong, “Deep learning methods applied to Higgs physics at the LHC”, Dec. (2020), PhD thesis under supervision of M. Gallinaro, Instituto Superior Técnico Universidade de Lisboa, CMS-TS-2021-024 ; CERN-THESIS-2021-211 | Sole author of: G. Strong, “On the impact of selected modern deep-learning techniques to the performance and celerity of classification models in an experimental high-energy physics use case”, Sep. (2020), Mach. Learn.: Sci. Technol. 1 045006 | Researcher for: T. Dorigo, J. Kieseler, L. Layer, G. Strong, “Muon Energy Measurement from Radiative Losses in a Calorimeter for a Collider Detector”, Aug. (2020), arXiv:2008.10958 [physics.ins-det] | Contributing presenter for: M. Gallinaro, et al. “Beyond the Standard Model in Vector Boson Scattering Signatures”, May (2020), arXiv:2005.09889 [hep-ph] | Researcher for di-Higgs sections of HL-LHC projections included in: “Higgs physics at the HL-LHC and HE-LHC”, Dec. (2019), CERN Yellow Reports: Monographs Physics of the HL-LHC, and perspectives at the HE-LHC, CYRM-2019-007.221 | ATLAS and CMS Collaborations, “Collection of notes from ATLAS and CMS”, Dec. (2019), CERN Yellow Reports: Monographs Physics of the HL-LHC, and perspectives at the HE-LHC CYRM-2019-007.Addendum, CERN-LPCC-2018-04 | CMS Collaboration, “A MIP Timing Detector for the CMS Phase-2 Upgrade”, Mar. (2019), CERN-LHCC-2019-003 CMS-TDR-020 | | Joint main author, and researcher for hh→bb𝜏𝜏 section of: CMS Collaboration, “Prospects for HH measurements at the HL-LHC”, Dec. (2018), CMS-PAS-FTR-18-019 | Joint main author, and researcher for h→𝜏𝜏 section of: AMVA4NewPhysics, “Classification and Regression Tools in Higgs Measurements”, Oct. (2018), AMVA4NewPhysics D1.4 | Contributed to the alignment and calibration studies for: CMS Collaboration and TOTEM Collaboration, “Observation of proton-tagged, central (semi)exclusive production of high-mass lepton pairs in pp collisions at 13 TeV with the CMS-TOTEM precision proton spectrometer,” JHEP 07, 153 (2018) | Contributed to central discussions of software, hardware, and tool requirements: Albertsson et al., “Machine Learning in High Energy Physics Community White Paper”, Jul. (2018), arXiv:1807.02876 [physics.comp-ph] | Joint main author, and researcher for hh→bb𝜏𝜏 section of: AMVA4NewPhysics, “Multivariate Analysis Methods for Higgs Boson Searches At The Large Hadron Collider”, Feb. (2017), AMVA4NewPhysics D1.1 | Sole author of: G. Strong, “Gluon splitting to b-quark pairs in proton-proton collisions at √s=8 TeV with ATLAS”, Sep. (2015), Master’s thesis undersupervision of A. Robson, A. Buckley, G Hesketh, &amp; J McFayden, University of Glasgow, glathesis:2015-6680 | Sole author of: G. Strong, “Top-tagger optimisation and comparison”, Apr. (2013), Master’s thesis undersupervision of F. Krauss, Durham University | Software . Core developer of: MODE collaboration, “TomOpt: Differential optimisation for muon tomography” | Main developer of: J Kieseler, G. Strong, F. Chiandotto, T. Dorigo, &amp; L. Layer, “Public version of code used in “Calorimetric Measurement of Multi-TeV Muons via Deep Regression””, Zenodo. Supporting code for The European Physical Journal C 82:79 | Sole developer of: G. Strong, “PyTorch INFERNO”, Mar. (2021), Zenodo, https://github.com/GilesStrong/pytorch_inferno | Sole developer of: G. Strong, “HiggsML Lumin”, Apr. (2020), Zenodo, https://github.com/GilesStrong/HiggsML_Lumin. Supporting code for Mach. Learn.: Sci. Technol. 1 045006 | Sole developer of: G. Strong, “PyTorch Tutorial”, Feb. (2020), Zenodo, https://github.com/GilesStrong/PyTorch_Tutorial | Core developer of: G. Strong, “LUMIN”, Mar. (2019), Zenodo, https://github.com/GilesStrong/lumin | Sole developer of: G. Strong, “LIP Data Science School in (astro)particle physics and cosmology, Braga 2019: Keras Tutorial”, Mar. 2019, Zenodo, https://github.com/GilesStrong/LIP_DSS_Keras_Tutorial_2019 | Sole develoepr of: G. Strong, “Project repository for IST QCD course, 2018 - Static-Quark Potential Calculations”, Dec. (2018), Zenodo, https://github.com/GilesStrong/LatticeQCD_IST2018 | Sole developer of: G. Strong, “Hyper-Parameter Optimisation Part I”, Zenodo, https://github.com/GilesStrong/Smith_HyperParams1_Demo | Datasets . Contributor to: J Kieseler, G. Strong, F. Chiandotto, T. Dorigo, &amp; L. Layer, “Preprocessed Dataset for ``Calorimetric Measurement of Multi-TeV Muons via Deep Regression”, Zenodo. Supporting data for The European Physical Journal C 82:79 | Creator of: G. Strong, “Di-Higgs ML Tutorial data”, Zenodo |",
          "url": "https://gilesstrong.github.io/website/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Presentations",
          "content": "Conferences . Muon energy regression hackathon challenge, T. Dorigo, L. Layer, G. Strong, Quarks to Cosmos with AI - Online/Carnegie Mellon University, USA, 12/07/21-16/07/21. Jointly involved in: organisation &amp; planning, baseline code, scoring, introductory &amp; technical seminars | . | Improvements to ML for Searches at the LHC, G. Strong, International Conference on High Energy Physics 2020 - Online, 28/07/20. | CMS HL-LHC Projection for Non-Resonant Di-Higgs Production in the bb𝝉𝝉 Decay Channel, M. Bengala, M. Gallinaro, R Santo, G. Strong (speaker), &amp; L. Cadamuro, Posters@LHCC - CERN, Switzerland, 27/02/19. | Recent developments in deep-learning applied to open HEP data, G. Strong, XIIIth Conference for Quark Confinement and the Hadron Spectrum - Maynooth University, Ireland, 02/08/18. | Recent developments in deep-learning applied to open HEP data, G. Strong, Advanced Statistics for Physics Discovery (poster session) - University of Padova, Italy, 25/09/18. | Seminars &amp; tutorials . TomOpt: Differential Muon Tomography Optimisation, G. Strong (speaker), T. Dorigo, SciComp Seminar Series TU Kaiserslautern - Online, 02/12/21 | Graph neural networks in HEP, R. Pedro (speaker) &amp; G. Strong (speaker), LIP competency centre for Simulation and Big Data meeting (10.5281/zenodo.5090549) - Online, 12/07/21. | TomOpt: Differential Muon Tomography Optimisation, G. Strong &amp; T. Dorigo, MODE meeting - Online, 07/06/21. | Inferno As A Drop-in* Loss Function, G. Strong, gradHEP Campfire #2 - Online, 12/02/21. | Deep Learning Methods Applied to Higgs Physics at The LHC, G. Strong, PhD Defense Instituto Superior Técnico Universidade de Lisboa - Online, 22/12/20. | Improvements to ML for Searches at the LHC, G. Strong, Geneva Data Mining and Machine Learning group - Online, 20/05/20. | PyTorch Tutorial, G. Strong, 6ªs Jornadas de Engenharia Física - IST, Portugal, 04/03/20. | Machine learning in HEP, G. Strong, 40º PubhD de Lisboa: Especial “Infinitamente Pequeno” - Má Língua, Portugal, 04/12/20. | A Dive Into Deep Reinforcement Learning, G. Strong (summarising: OpenAI - Spinning Up in Deep RL), LIP Seminar Series - LIP, Portugal, 03/10/19. | Understanding Neural Networks, G. Strong, LIP Summer Tutorials - LIP-Lisbon, Portugal, 17/08/19. | Keras Tutorial, G. Strong, LIP Data Science in (Astro)Particle Physics and Cosmology - University of Minho, Portugal, 26/03/19-27/03/19. | Hyper-Parameter Optimisation Part I, G. Strong (summarising: L. Smith arXiv:1803.09820), 7th AMVA4NewPhysics Workshop - LIP-Lisbon, Portugal, 31/10/18. | Understanding Neural Networks, G. Strong, LIP Summer Tutorials - LIP, Portugal, 11/07/18. | Hyper-Parameter Optimisation Part I, G. Strong (summarising: L. Smith arXiv:1803.09820), LIP competency centre for Simulation and Big Data meeting - Online, 06/07/18. | Keras Tutorial, G. Strong, LIP Data Science in (Astro)Particle Physics and Cosmology - LIP-Lisbon, Portugal, 13/03/18. | Keras Tutorial, G. Strong, LIP competency centre for Simulation and Big Data meeting - Online, 14/07/17. | Understanding Neural Networks, G. Strong, LIP Seminar Series - LIP, Portugal, 15/03/17. | Workshops &amp; meetings . Differential programming for detector optimisation, G. Strong on behalf of MODE Collaboration, Analysis Ecosystems Workshop II - Paris/Online, France, 23/05/22 | Two-level graphs for muon-tomography inference, G. Strong, 5th IML Workshop - CERN/Online, 13/05/22 | TomOpt: PyTorch-based Differential Muon Tomography Optimisation, G. Strong (speaker), T. Dorigo, 24th European Automatic Differentiation Workshop - Online, 03/11/21 | TomOpt: PyTorch-based Differential Muon Tomography Optimisation, G. Strong (speaker), T. Dorigo, First MODE Workshop on Differentiable Programming - Université Catholique de Louvain/Online, Belgium, 06/09/21 | An Introduction to LUMIN: A deep learning and data science ecosystem for high-energy physics, G. Strong, CMS Town Hall 2021 (10.5281/zenodo.5089905) - Online, 29/07/21. | An Introduction to LUMIN: A deep learning and data science ecosystem for high-energy physics, G. Strong, PyHEP 2021 (10.5281/zenodo.5089905) - Online, 09/07/21. | Calorimetric Measurement of Multi-TeV Muons via Deep Regression, J Kieseler, G. Strong (speaker), F. Chiandotto, T. Dorigo, &amp; L. Layer, ML4Jets - Heidelberg/Online, Germany, 07/07/21. | PyTorch INFERNO, G. Strong, PyHEP 2021 (10.5281/zenodo.5075081) - Online, 06/07/21. | Improvements To Lumin &amp; Inferno In PyTorch, G. Strong, IML Meeting - Online, 16/03/21 | LUMIN, G. Strong, INFN-ML Meeting - Online, 27/07/20. | LUMIN, G. Strong, CMS ML Kick-Off Workshop - Online, 02/07/20. | Neural Networks On A Budget, G. Strong, LIP &amp; IDPASC Students’ Workshop - Online, 25/06/20. | Machine Learning In High Energy Physics, G. Strong, VBScan Workshop - LIP-Lisbon, Portugal, 05/12/19. | Deep Learning In Higgs Physics, G. Strong, 8th AMVA4NP Workshop - DESY, Germany, 06/07/19. | Deep Learning In Higgs Physics, G. Strong, LIP &amp; IDPASC Students’ Workshop - University of Minho, Portugal, 01/07/19. | Attempted Reproduction of the Super-TML Method, G. Strong, 3rd CMS ML Workshop - CERN, Switzerland, 19/06/19. | Coopetition, G. Strong, AMVA4NewPhysics Seminar Series - Online, 29/04/19. | LUMIN, G. Strong, 3rd IML Workshop - CERN, Switzerland, 17/04/19. | Hunting Supersymmetry &amp; Higgs with the CMS detector, D. Bastos, P. Bargasa (speaker), N. Leonardo, L. Sintra, &amp; G. Strong, ENEF2019 Inside Views - LIP-Lisbon, Portugal, 27/02/19. | hh→bb𝜏𝜏 at the HL-LHC YR Studies, M. Bengala, M. Gallinaro, R Santo, and G. Strong (speaker), CMS hh→bb𝜏𝜏 Workshop - LLR-Paris, France, 21/02/19. | Recent developments in deep-learning applied to open HEP data, G. Strong, IML Meeting - CERN, Switzerland, 30/11/18. | Progress Report, G. Strong, 7th AMVA4NewPhysics Workshop - LIP-Lisbon, Portugal, 01/11/18. | Recent developments in deep-learning applied to open HEP data, G. Strong, 3rd ATLAS ML Workshop (public session invited speaker) - CERN, Switzerland, 17/10/18. | DNN classifiers in di-Higgs searches, G. Strong, 6th AMVA4NewPhysics Workshop - National and Kapodistrian University of Athens, Greece, 21/06/18. | DNNs in hh classification, G. Strong, 3rd CMS hh Workshop - LLR-Paris, France, 05/04/18. | Applications of Machine Learning to Particle Physics, G. Strong, 3rd LIP Mini-school of particle physics - Oeiras, Portugal, 07/02/18. | DNNs in regression and classification, G. Strong, CMS hh→bb𝜏𝜏 Workshop - INFN-Pisa, Italy, 16/01/18. | DNN for Tau Isolation, G. Strong on behalf of CMS Tau POG, CMS ML Forum Meeting - CERN, Switzerland, 22/11/17. | Regression &amp; classification of di-Higgs events, G. Strong, AMVA4NewPhysics Mid-term review - CP3, Belgium, 30/08/17. | 2017 Alignment of CT-PPS, G. Strong (speaker) &amp; J. Kaspar, CT-PPS Physics Meeting - 30/08/17. | Feature optimisation through regression, G. Strong, 4th AMVA4NewPhysics Workshop - Oviedo University, Spain, 18/05/17. | Machine Learning in Higgs Physics, G. Strong, LIP PhD-Students’ Workshop - Coimbra University, Portugal, 25/03/17. | Machine Learning in Higgs Physics, G. Strong, Lisbon mini-school on Particle and Astro-particle Physics - Sesimbra, Portugal, 08/02/17. | Regression Studies in di-Higgs Events, G. Strong, 3rd AMVA4NewPhysics Workshop - University of Oxford, UK, 20/12/16. |",
          "url": "https://gilesstrong.github.io/website/presentations/",
          "relUrl": "/presentations/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "Other contributions",
          "content": "Supervision . Miguel Bengala and Rodrigo Santo, IST, University of Lisbon: LIP summer internships 2018. Projection study for CMS sensitivity to non-resonant di-Higgs production at the HL-LHC with deep neural networks. Contributed to CMS-FTR-18-019, CERN-LPCC-2018-04, CMS-TDR-020. Studies published in CERN Yellow Report CYRM-2019-007.221 &amp; CYRM-2019-007.Addendum | Antonio Capela and Gonçalo Paulo, IST, University of Lisbon: LIP summer internships 2017. Study of classification of resonant and non-resonant di-Higgs production with deep neural networks. | Ricardo Barrué, António Costa, and João Gonçalves, IST, University of Lisbon. LIP summer internships 2016. Study of classification of resonant di-Higgs production with deep neural networks. | . Scientific outreach and public engagement . Continual blogging at gilesstrong.github.io/website/, and author of 42 blog posts over at amva4newphysics.wordpress.com. Mainly covering academic life and technical explanations of both physics and machine learning. | Participating scientist in “I’m a Scientist Get Me Out of Here” (General Science - 2021 round) - a public engagement and scientific outreach initiative enabling school students to chat live with scientists about their work and what being a scientist is like. | Joint main presenter at the 40th PubhD de Lisboa, speaking on Machine learning in HEP to the general public | Attendee of the AMVA4NewPhysics and INSIGHTS Outreach Workshop - CERN, Switzerland, September 2018. | Co-moderator for a results presentation and discussion between high schools during the 2018 International Physics Masterclasses. | Attendee of the International School of Scientific Journalism - Erice, Italy, July 2017. | Assistant at outreach events in two schools in Venice, February 2017. |",
          "url": "https://gilesstrong.github.io/website/contributions/",
          "relUrl": "/contributions/",
          "date": ""
      }
      
  

  

  

  

  

  
  

  
  

  
  

  
  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gilesstrong.github.io/website/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}